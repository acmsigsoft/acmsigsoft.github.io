
<!doctype html>
<head>
<META http-equiv="Content-Style-Type" content="text/css">
<title>SBST '18- Proceedings of the 11th International Workshop on Search-Based Software Testing</title>
<STYLE type="text/css">
#DLtoc {
	font: normal 12px/1.5em Arial, Helvetica, sans-serif;
	}

#DLheader {
	}
#DLheader h1 {
	font-size:16px;	
}
	
#DLcontent {
	 font-size:12px;
	}
#DLcontent h2 {
	 font-size:14px;
	 margin-bottom:5px;
	}
#DLcontent h3 {
	 font-size:12px;
	 padding-left:20px;
	 margin-bottom:0px;
	}

#DLcontent ul{
	margin-top:0px;
	margin-bottom:0px;
	}
		
.DLauthors li{
	display: inline;
	list-style-type: none;
	padding-right: 5px;
	}
	
.DLauthors li:after{
	content:",";
	}
.DLauthors li.nameList.Last:after{
	content:"";
	}		

.DLabstract {
	 padding-left:40px;
	 padding-right:20px;
	 display:block;
	}

.DLformats li{
	display: inline;
	list-style-type: none;
	padding-right: 5px;
	}
	
.DLformats li:after{
	content:",";
	}
.DLformats li.formatList.Last:after{
	content:"";
	}		

.DLlogo {
	vertical-align:middle; 
	padding-right:5px;
	border:none;
	}
	
.DLcitLink {
	margin-left:20px;
	}	

.DLtitleLink {
	margin-left:20px;
	}	

.DLotherLink {
	margin-left:0px;
	}		
   
</STYLE>
</head>
<body>
<div id="DLtoc">
<div id="DLheader">
<h1>SBST '18- Proceedings of the 11th International Workshop on Search-Based Software Testing</h1>
<a class="DLcitLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_citation.cfm-3Fid-3D3194718&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=BbN9D_v4N_1aKx24MkR1A2tM3wAI0aMwq9Nuawz_xL8&e=" title="Go to the ACM Digital Library for additional information about this proceeding"><img class="DLlogo" src="https://dl.acm.org/img/dllogo.png" alt="Digital Library logo" height="30" width="30">Full Citation in the ACM Digital Library</a>
</div>
<div id="DLcontent">
<h2>SESSION: Keynote I</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653558&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=ds3P8G3k-0Jl6bFkS0eNjWN1S63-VLhGmbN09JZWf4Q&e=" title="Get the Full Text from the ACM Digital Library">Predictive analytics for software testing: keynote paper</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Federica Sarro</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>This keynote discusses the use of Predictive Analytics for Software Engineering, and in particular for Software Defect Prediction and Software Testing, by presenting the latest results achieved in these fields leveraging Artificial Intelligence, Search-based and Machine Learning methods, and by giving some directions for future work.</p></div> </div>
<h2>SESSION: Technical session I</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653559&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=YgDdO0Bezd4MdUkslUyc1WAI1AvbfRiVyWov-nZ4aGY&e=" title="Get the Full Text from the ACM Digital Library">Multifaceted test suite generation using primary and supporting fitness functions</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Gregory Gay</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Dozens of criteria have been proposed to judge testing adequacy. Such criteria are important, as they guide automated generation efforts. Yet, the current use of such criteria in automated generation contrasts how such criteria are used by humans. For a human, coverage is part of a <i>multifaceted</i> combination of testing strategies. In automated generation, coverage is typically <i>the</i> goal, and a single fitness function is applied at one time. We propose that the key to improving the fault detection efficacy of search-based test generation approaches lies in a targeted, multifaceted approach pairing <i>primary</i> fitness functions that effectively explore the structure of the class under test with lightweight <i>supporting</i> fitness functions that target particular scenarios likely to trigger an observable failure.</p> <p>This report summarizes our findings to date, details the hypothesis of primary and supporting fitness functions, and identifies outstanding research challenges related to multifaceted test suite generation. We hope to inspire new advances in search-based test generation that could benefit our software-powered society.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653550&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=tVufQD3TQk2DlGCh_PFHpNTH86c--moa95z-Dma2sFs&e=" title="Get the Full Text from the ACM Digital Library">Search-based optimization for the testing resource allocation problem: research trends and opportunities</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Roberto Pietrantuono</li>
<li class="nameList Last">Stefano Russo</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>This paper explores the usage of search-based techniques for the Testing Resource Allocation Problem (TRAP). We focus on the analysis of the literature, surveying the research proposals where search-based techniques are exploited for different formulations of the TRAP. Three dimensions are considered: the model formulation, solution, and validation. The analysis allows to derive several observations, and finally outline some new research directions towards better (namely, closer to real-world settings) modelling and solutions, highlighting the most promising areas of investigation.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653551&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=v_1LhQRzpKsFJDIk7uzRN-mNkgK1n9wAGEQy1egnV8Q&e=" title="Get the Full Text from the ACM Digital Library">An effective approach for regression test case selection using pareto based multi-objective harmony search</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Ankur Choudhary</li>
<li class="nameList">Arun Prakash Agrawal</li>
<li class="nameList Last">Arvinder Kaur</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Regression testing is a way of catching bugs in new builds and releases to avoid the product risks. Corrective, progressive, retest all and selective regression testing are strategies to perform regression testing. Retesting all existing test cases is one of the most reliable approaches but it is costly in terms of time and effort. This limitation opened a scope to optimize regression testing cost by selecting only a subset of test cases that can detect faults in optimal time and effort. This paper proposes Pareto based Multi-Objective Harmony Search approach for regression test case selection from an existing test suite to achieve some test adequacy criteria. Fault coverage, unique faults covered and algorithm execution time are utilised as performance measures to achieve optimization criteria. The performance evaluation of proposed approach is performed against Bat Search and Cuckoo Search optimization. The results of statistical tests indicate significant improvement over existing approaches.</p></div> </div>
<h2>TUTORIAL SESSION: Tutorial</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653552&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=AZGl44Ui883DssYRbw7CUBap1O9L8X8BglUwDVzxFCE&e=" title="Get the Full Text from the ACM Digital Library">Evaluating search-based techniques with statistical tests</a>
</h3>
<ul class="DLauthors">
 <li class="nameList Last">Andre Arcuri</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>This tutorial covers the basics of how to use statistical tests to evaluate and compare search-algorithms, in particular when applied on software engineering problems. Search-algorithms like Hill Climbing and Genetic Algorithms are randomised. Running such randomised algorithms twice on the same problem can give different results. It is hence important to run such algorithms multiple times to collect average results, and avoid so publishing wrong conclusions that were based on just luck. However, there is the question of how often such runs should be repeated. Given a set of <i>n</i> repeated experiments, is such <i>n</i> large enough to draw sound conclusions? Or should had more experiments been run? Statistical tests like the Wilcoxon-Mann-Whitney U-test can be used to answer these important questions.</p></div> </div>
<h2>SESSION: Tool competition</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653553&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=0V878xcSaZXz1jH-iElVWTIoU5pGuvomntwZBoTXyRE&e=" title="Get the Full Text from the ACM Digital Library">Java unit testing tool competition: sixth round</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Urko Rueda Molina</li>
<li class="nameList">Fitsum Kifetew</li>
<li class="nameList Last">Annibale Panichella</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>We report on the advances in this sixth edition of the JUnit tool competitions. This year the contest introduces new benchmarks to assess the performance of JUnit testing tools on different types of real-world software projects. Following on the statistical analyses from the past contest work, we have extended it with the combined tools performance aiming to beat the human made tests. Overall, the 6th competition evaluates four automated JUnit testing tools taking as baseline human written test cases for the selected benchmark projects. The paper details the modifications performed to the methodology and provides full results of the competition.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653554&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=Zwzsw7aTo_zNR-CIwG9VfZRztu9s5Z6jO3cvc9WxxU4&e=" title="Get the Full Text from the ACM Digital Library">T3 @SBST2018 benchmark, and how much we can get from asemantical testing</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">I. S. W. B. Prasetya</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>This paper discusses the performance of the automated testing tool for Java called T3 and compares it with few other tools and human written tests in a benchmark set by the Java Unit Testing Tool Contest 2018. Since all the compared tools rely on randomization when generating their test data, albeit to different degrees and with different heuristics, this paper also tries to give some insight on just how far we can go without having to reconstruct the precise semantic of the programs under test in order to test them.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653565&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=y6ttbBwgI2bYMJUCC99tk3w-qSM7BnjqzCOtS1bUE1c&e=" title="Get the Full Text from the ACM Digital Library">E<scp>vo</scp>s<scp>uite</scp> at the SBST 2018 tool competition</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Gordon Fraser</li>
<li class="nameList">Jos&#233; Miguel Rojas</li>
<li class="nameList Last">Andrea Arcuri</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>E<scp>vo</scp>S<scp>uite</scp> is a search-based tool that automatically generates executable unit tests for Java code (JUnit tests). This paper summarises the results and experiences of E<scp>vo</scp>S<scp>uite</scp>'s participation at the sixth unit testing competition at SBST 2018, where E<scp>vo</scp>S<scp>uite</scp> achieved the highest overall score (687 points) for the fifth time in six editions of the competition.</p></div> </div>
<h2>SESSION: Keynote II</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653566&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=cfx7-LZLC0BMWJQizAVLt8Uwb6QPwuB43BOzMIG_N6s&e=" title="Get the Full Text from the ACM Digital Library">Testing and continuous integration at scale: limits, costs, and expectations</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Kim Herzig</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Build and verification systems and processes have changed significantly in the past decade and so did the corresponding development processes. In fact, build and verification systems are a lower bound of how fast a company can ship new features---the more time spent to compile and test code the worse the company's ability to compete on the market. In other words, verification is key but needs to be affordable---in terms of money but also in terms of time. This keynote will be about some fundamental concepts of modern industry software testing, their cost and limits as well as expectations of software developers and teams against build, test, and development processes.</p></div> </div>
<h2>SESSION: Technical session II</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653567&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=hSbxS-NicS2g3mJWDblLhxLT3HrZSlOWaLzdL4Tz9Ss&e=" title="Get the Full Text from the ACM Digital Library">From operational to declarative specifications using a genetic algorithm</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Facundo Molina</li>
<li class="nameList">Renzo Degiovanni</li>
<li class="nameList">Germ&#225;n Regis</li>
<li class="nameList">Pablo Castro</li>
<li class="nameList">Nazareno Aguirre</li>
<li class="nameList Last">Marcelo Frias</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>In specification-based test generation, sometimes having a formal specification is not sufficient, since the specification may be in a different formalism from that required by the generation approach being used. In this paper, we deal with this problem specifically in the context in which, while having a formal specification in the form of an operational invariant written in a sequential programming language, one needs, for test generation, a declarative invariant in a logical formalism. We propose a genetic algorithm that given a catalog of common properties of invariants, such as acyclicity, sortedness and balance, attempts to evolve a conjunction of these that most accurately approximates an original operational specification. We present some details of the algorithm, and an experimental evaluation based on a benchmark of data structures, for which we evolve declarative logical invariants from operational ones.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653568&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=JiYzPKWVk7uJHWHV-n0RLjL2zN23Z82fwgTeG7qsrvA&e=" title="Get the Full Text from the ACM Digital Library">To call, or not to call: contrasting direct and indirect branch coverage in test generation</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Gregory Gay</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>While adequacy criteria offer an end-point for testing, they do not mandate <i>how</i> targets are covered. Branch Coverage may be attained through <i>direct</i> calls to methods, or through <i>indirect</i> calls between methods. Automated generation is biased towards the rapid gains offered by indirect coverage. Therefore, even with the same end-goal, humans and automation produce very different tests. Direct coverage may yield tests that are more understandable, and that detect faults missed by traditional approaches. However, the added burden for the generation framework may result in lower coverage and faults that emerge through method interactions may be missed.</p> <p>To compare the two approaches, we have generated test suites for both, judging efficacy against real faults. We have found that requiring direct coverage results in lower achieved coverage and likelihood of fault detection. However, both forms of Branch Coverage cover code and detect faults that the other does not. By isolating methods, Direct Branch Coverage is less constrained in the choice of input. However, traditional Branch Coverage is able to leverage method interactions to discover faults. Ultimately, both are situationally applicable within the context of a broader testing strategy.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653569&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=Z5_EqG2JHM7z-8yrbM1x8KkmKoOAg5h8vPMH9OhiODo&e=" title="Get the Full Text from the ACM Digital Library">Generating test input with deep reinforcement learning</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Junhwi Kim</li>
<li class="nameList">Minhyuk Kwon</li>
<li class="nameList Last">Shin Yoo</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed meta-heuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present G<scp>un</scp>P<scp>owder</scp>, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the effectiveness of our approach by conducting a small empirical study. Finally, we find that agents can learn metaheuristic algorithms for SBST, achieving 100% branch coverage for training functions. Our study sheds light on the future integration of deep neural network and SBST.</p></div> </div>
<h2>SESSION: Technical session III</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653560&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=FVRg2nFyl8i2n-HS0a338oxOSzZGRJ0Vurc0BwQkgPc&e=" title="Get the Full Text from the ACM Digital Library">An empirical analysis of the mutation operator for run-time adaptive testing in self-adaptive systems</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Erik M. Fredericks</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>A self-adaptive system (SAS) can reconfigure at run time in response to uncertainty and/or adversity to continually deliver an acceptable level of service. An SAS can experience uncertainty during execution in terms of environmental conditions for which it was not explicitly designed as well as unanticipated combinations of system parameters that result from a self-reconfiguration or misunderstood requirements. Run-time testing provides assurance that an SAS continually behaves as it was designed even as the system reconfigures and the environment changes. Moreover, introducing adaptive capabilities via lightweight evolutionary algorithms into a run-time testing framework can enable an SAS to effectively update its test cases in response to uncertainty alongside the SAS's adaptation engine while still maintaining assurance that requirements are being satisfied. However, the impact of the evolutionary parameters that configure the search process for run-time testing may have a significant impact on test results. Therefore, this paper provides an empirical study that focuses on the mutation parameter that guides online evolution as applied to a run-time testing framework, in the context of an SAS.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN653561&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=MU9I06QDFRpI4D138if2Ny7-gEcxgSrcezRxdKmkCxE&s=jW81RjhV0lxDzzTpu-YKKZuw0MWKAYdKCi-DsxWzBn0&e=" title="Get the Full Text from the ACM Digital Library">On the effect of object redundancy elimination in randomly testing collection classes</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Pablo Ponzio</li>
<li class="nameList">Valeria Bengolea</li>
<li class="nameList">Sim&#243;n Guti&#233;rrez Brida</li>
<li class="nameList">Gast&#243;n Scilingo</li>
<li class="nameList">Nazareno Aguirre</li>
<li class="nameList Last">Marcelo Frias</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>In this paper, we analyze the effect of reducing object redundancy in random testing, by comparing the Randoop random testing tool with a version of the tool that disregards tests that only produce objects that have been previously generated by other tests. As a side effect, this variant also identifies methods in the software under test that never participate in state changes, and uses these more heavily when building assertions.</p> <p>Our evaluation of this strategy concentrates on <i>collection classes</i>, since in this context of object-oriented implementations that describe <i>stateful</i> objects obbeying complex invariants, object variability is highly relevant. Our experimental comparison takes the main data structures in java.util, and shows that our object redundancy reduction strategy has an important impact in testing collections, measured in terms of code coverage and mutation killing.</p></div> </div>
</div>
</div>
</body>
</html>
