
<!doctype html>
<head>
<META http-equiv="Content-Style-Type" content="text/css">
<title>AST '18- Proceedings of the 13th International Workshop on Automation of Software Test</title>
<STYLE type="text/css">
#DLtoc {
	font: normal 12px/1.5em Arial, Helvetica, sans-serif;
	}

#DLheader {
	}
#DLheader h1 {
	font-size:16px;	
}
	
#DLcontent {
	 font-size:12px;
	}
#DLcontent h2 {
	 font-size:14px;
	 margin-bottom:5px;
	}
#DLcontent h3 {
	 font-size:12px;
	 padding-left:20px;
	 margin-bottom:0px;
	}

#DLcontent ul{
	margin-top:0px;
	margin-bottom:0px;
	}
		
.DLauthors li{
	display: inline;
	list-style-type: none;
	padding-right: 5px;
	}
	
.DLauthors li:after{
	content:",";
	}
.DLauthors li.nameList.Last:after{
	content:"";
	}		

.DLabstract {
	 padding-left:40px;
	 padding-right:20px;
	 display:block;
	}

.DLformats li{
	display: inline;
	list-style-type: none;
	padding-right: 5px;
	}
	
.DLformats li:after{
	content:",";
	}
.DLformats li.formatList.Last:after{
	content:"";
	}		

.DLlogo {
	vertical-align:middle; 
	padding-right:5px;
	border:none;
	}
	
.DLcitLink {
	margin-left:20px;
	}	

.DLtitleLink {
	margin-left:20px;
	}	

.DLotherLink {
	margin-left:0px;
	}		
   
</STYLE>
</head>
<body>
<div id="DLtoc">
<div id="DLheader">
<h1>AST '18- Proceedings of the 13th International Workshop on Automation of Software Test</h1>
<a class="DLcitLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_citation.cfm-3Fid-3D3194733&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=vwWiPKCi6A2oTfitkBuTX3ckqufR0MwSWRu35G1Y7SY&e=" title="Go to the ACM Digital Library for additional information about this proceeding"><img class="DLlogo" src="https://dl.acm.org/img/dllogo.png" alt="Digital Library logo" height="30" width="30">Full Citation in the ACM Digital Library</a>
</div>
<div id="DLcontent">
<h2>SESSION: Keynote 1</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652953&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=hb9DlPONRj0GOEB3o1Ce89gK1KB4AyG95w8HYzea1P4&e=" title="Get the Full Text from the ACM Digital Library">Software testing as a problem of machine learning: towards a foundation on computational learning theory (extended abstract of keynote speech)</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Hong Zhu</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>In recent years, the application of machine learning techniques to software testing has been an active research area. Among the most notable work reported in the literature are those experiments on the uses of supervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1]. Experiment data show that it seems a promising approach to the test oracle automation problem. In general, software testing is an inductive inference in the course of which the tester attempts to deduce general properties of a software system by observing the behaviours of the system on a finite number of test cases [2]. Thus, there is a great potential for the application of machine learning to software testing.</p></div> </div>
<h2>SESSION: Test models</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652954&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=KPxW-1y_NvDlDXxZkcVEYEYExuiTxD77K_x0M2FKnyE&e=" title="Get the Full Text from the ACM Digital Library">An automated model-based test oracle for access control systems</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Antonia Bertolino</li>
<li class="nameList">Said Daoudagh</li>
<li class="nameList">Francesca Lonetti</li>
<li class="nameList Last">Eda Marchetti</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>In the context of XACML-based access control systems, an intensive testing activity is among the most adopted means to assure that sensible information or resources are correctly accessed. Unfortunately, it requires a huge effort for manual inspection of results: thus automated verdict derivation is a key aspect for improving the cost-effectiveness of testing. To this purpose, we introduce XACMET, a novel approach for automated model-based oracle definition. XACMET defines a typed graph, called the XAC-Graph, that models the XACML policy evaluation. The expected verdict of a specific request execution can thus be automatically derived by executing the corresponding path in such graph. Our validation of the XACMET prototype implementation confirms the effectiveness of the proposed approach.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652965&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=4kLMgUBoHADruBzYtuNuDhLb1qv_IpMocEH1gbFbrIc&e=" title="Get the Full Text from the ACM Digital Library">Testing service oriented architectures using stateful service visualization via machine learning</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Hasan Ferit Eni&#351;er</li>
<li class="nameList Last">Alper Sen</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Today's enterprise software systems are much complicated than the past. Increasing number of dependent applications, heterogeneous technologies and wide usage of Service Oriented Architectures (SOA), where numerous services communicate with each other, makes testing of such systems challenging. For testing these software systems, the concept of service virtualization is gaining popularity. Service virtualization is an automated technique to mimic the behavior of a given real service. Services can be classified as stateless or stateful services. Many services are stateful in nature. Although there are works in the literature for virtualization of state-less services, no such solution exists for stateful services. To the best of our knowledge, this is the first work for stateful service virtualization. We employ classification based and sequence-to-sequence based machine learning algorithms in developing our solutions. We demonstrate the validity of our approach on two data sets collected from real life services and obtain promising results.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652966&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=8OssQF3CmFUuZ9M7RqqQZPPmtWaiGdBzlV4EmDtxhB4&e=" title="Get the Full Text from the ACM Digital Library">Revisiting AI and testing methods to infer FSM models of black-box systems</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Roland Groz</li>
<li class="nameList">Adenilso Simao</li>
<li class="nameList">Nicolas Bremond</li>
<li class="nameList Last">Catherine Oriat</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Machine learning in the form of inference of state machine models has gained popularity in model-based testing as a means of retrieving models from software systems. By combining an old idea from machine inference with methods from automata testing in a heuristic approach, we propose a new promising direction for inferring black box systems that cannot be reset. Preliminary experiments show that this heuristic approach scales up well and outperforms more systematic approaches.</p></div> </div>
<h2>SESSION: Mobile app testing</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652967&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=vK80fzl-vuu8COzuLLyX__Q2bSb0lYfOESrC2YbeStE&e=" title="Get the Full Text from the ACM Digital Library">Planning-based security testing of web applications</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Josip Bozic</li>
<li class="nameList Last">Franz Wotawa</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Web applications are deployed on machines around the globe and offer almost universal accessibility. The systems ensure functional interconnectivity between different components on a 24/7 basis. One of the most important requirements represents data confidentiality and secure authentication. However, implementation flaws and unfulfilled requirements can result in security leaks that can be eventually exploited by a malicious user. Here different testing methods are applied in order to detect software defects and prevent unauthorized access in advance.</p> <p>Automated planning and scheduling provides the possibility to specify a specific problem and to generate plans, which in turn guide the execution of a program. In this paper, a planning-based approach is introduced for modeling and testing of web applications. The specification offers a high degree of extendibility and configurability but overcomes the limits of traditional graphical representations as well. In this way, new testing possibilities emerge that eventually lead to better vulnerability detection, thereby ensuring more secure services.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652968&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=vlKZy4jSKhJA_812haPheNzYkf0VwyPbs11l3p5WJ5I&e=" title="Get the Full Text from the ACM Digital Library">S<scp>entinel</scp>: generating GUI tests for Android sensor leaks</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Haowei Wu</li>
<li class="nameList">Yan Wang</li>
<li class="nameList Last">Atanas Rountev</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Due to the widespread use of Android devices and apps, it is important to develop tools and techniques to improve app quality and performance. Our work focuses on a problem related to hardware sensors on Android devices: the failure to disable unneeded sensors, which leads to <i>sensor leaks</i> and thus battery drain. We propose the S<scp>entinel</scp> testing tool to uncover such leaks. The tool performs static analysis of app code and produces a model which maps GUI events to callback methods that affect sensor behavior. The model is traversed to identify paths that are likely to exhibit sensor leaks during run-time execution. The reported paths are then used to generate test cases. The execution of each test case tracks the run-time behavior of sensors and reports observed leaks. Our experimental results indicate that S<scp>entinel</scp> effectively detects sensor leaks, while focusing the testing efforts on a very small subset of possible GUI event sequences.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652969&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=MpiMfAXnCp7HfxdCGuhSbts_kmKNcCKu0EH3j19kbR0&e=" title="Get the Full Text from the ACM Digital Library">On the effectiveness of random testing for Android: or how i learned to stop worrying and love the monkey</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Priyam Patel</li>
<li class="nameList">Gokul Srinivasan</li>
<li class="nameList">Sydur Rahaman</li>
<li class="nameList Last">Iulian Neamtiu</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Random testing of Android apps is attractive due to ease-of-use and scalability, but its effectiveness could be questioned. Prior studies have shown that Monkey - a simple approach and tool for random testing of Android apps - is surprisingly effective, "beating" much more sophisticated tools by achieving higher coverage. We study how Monkey's parameters affect code coverage (at class, method, block, and line levels) and set out to answer several research questions centered around improving the effectiveness of Monkey-based random testing in Android, and how it compares with manual exploration. First, we show that random stress testing via Monkey is extremely efficient (85 seconds on average) and effective at crashing apps, including 15 widely-used apps that have millions (or even billions) of installs. Second, we vary Monkey's event distribution to change app behavior and measured the resulting coverage. We found that, except for isolated cases, altering Monkey's default event distribution is unlikely to lead to higher coverage. Third, we manually explore 62 apps and compare the resulting coverages; we found that coverage achieved via manual exploration is just 2--3% higher than that achieved via Monkey exploration. Finally, our analysis shows that coarse-grained coverage is highly indicative of fine-grained coverage, hence coarse-grained coverage (which imposes low collection overhead) hits a performance vs accuracy sweet spot.</p></div> </div>
<h2>SESSION: Keynote 2</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652960&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=Poktqsj22V0iXOocWdwrkgY8YiXyjODi9LdKtt-4qZQ&e=" title="Get the Full Text from the ACM Digital Library">Towards software-defined and self-driving cloud infrastructure: extended abstract</a>
</h3>
<ul class="DLauthors">
<li class="nameList Last">Wei Xu</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Traditionally, people abstract away the infrastructure operation, such as power management, network traffic engineering and even the "cloud computing" layers from software developers. This abstraction brings easier application development and maintenance but leads to complexities and inefficiencies for infrastructure operation. Now in the data center community, people have adopted the software-defined paradigm, hoping to bring more flexibility to data center infrastructure, to improve the performance, efficiency, and reliability of the resource-demanding applications.</p></div> </div>
<h2>SESSION: System testing</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652961&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=KQZUc7mm2z6sb3z_eT04keBU0JqmWfPhk3sUky8vx_Y&e=" title="Get the Full Text from the ACM Digital Library">Improving continuous integration with similarity-based test case selection</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Francisco G. de Oliveira Neto</li>
<li class="nameList">Azeem Ahmad</li>
<li class="nameList">Ola Leifler</li>
<li class="nameList">Kristian Sandahl</li>
<li class="nameList Last">Eduard Enoiu</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Automated testing is an essential component of Continuous Integration (CI) and Delivery (CD), such as scheduling automated test sessions on overnight builds. That allows stakeholders to execute entire test suites and achieve exhaustive test coverage, since running all tests is often infeasible during work hours, i.e., in parallel to development activities. On the other hand, developers also need test feedback from CI servers when pushing changes, even if not all test cases are executed. In this paper we evaluate similarity-based test case selection (SBTCS) on integration-level tests executed on continuous integration pipelines of two companies. We select test cases that maximise diversity of test coverage and reduce feedback time to developers. Our results confirm existing evidence that SBTCS is a strong candidate for test optimisation, by reducing feedback time (up to 92% faster in our case studies) while achieving full test coverage using only information from test artefacts themselves.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652962&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=zlORfs8KSlNrXsYngJlOmmDCDDwWq_RYMqYhRkNsANI&e=" title="Get the Full Text from the ACM Digital Library">Memory corruption detecting method using static variables and dynamic memory usage</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Jihyun Park</li>
<li class="nameList">Changsun Park</li>
<li class="nameList">Byoungju Choi</li>
<li class="nameList Last">Gihun Chang</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Memory fault detection has been continuously studied and various detection methods exist. However, there are still remains many memory defects that are difficult to debug. Memory corruption is one of those defects that often cause a system crash. However, there are many cases where the location of the crash is different from the actual location causing the actual memory corruption. These defects are difficult to solve by existing methods.</p> <p>In this paper, we propose a method to detect real time memory defects by using static global variables derived from execution binary file and dynamic memory usage obtained by tracing memory related functions. We implemented the proposed method as a tool and applied it to the application running on the IoTivity platform. Our tool detects defects very accurately with low overhead even for those whose detected location and the location of its cause are different.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652963&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=Ym7mlMjE5W_1z67DyyJ4dMzS0kCA7X5-NpLZqthW3ds&e=" title="Get the Full Text from the ACM Digital Library">Guided test case generation through AI enabled output space exploration</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Christof Budnik</li>
<li class="nameList">Marco Gario</li>
<li class="nameList">Georgi Markov</li>
<li class="nameList Last">Zhu Wang</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Black-box software testing is a crucial part of quality assurance for industrial products. To verify the reliable behavior of software intensive systems, testing needs to ensure that the system produces the correct outputs from a variety of inputs. Even more critical, it needs to ensure that unexpected corner cases are tested. Existing approaches attempt to address this problem by the generation of input data to known outputs based on the domain knowledge of an expert. Such input space exploration, however, does not guarantee an adequate coverage of the output space as the test input data generation is done independently of the system output. The paper discusses a novel test case generation approach enabled by neural networks which promises higher probability of exposing system faults by systematically exploring the output space of the system under test. As such, the approach potentially improves the defect detection capability by identifying gaps in the test suite of uncovered system outputs. These gaps are closed by automatically determining inputs that lead to specific outputs by performing backward reasoning on an artificial neural network. The approach is demonstrated on an industrial train control system.</p></div> </div>
<h2>SESSION: Mutation-based testing</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652964&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=T2fMj5EiuEiQMM2gBEM92-1ABkkOHH9dYw1UwtxxArU&e=" title="Get the Full Text from the ACM Digital Library">Using controlled numbers of real faults and mutants to empirically evaluate coverage-based test case prioritization</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">David Paterson</li>
<li class="nameList">Gregory M. Kapfhammer</li>
<li class="nameList">Gordon Fraser</li>
<li class="nameList Last">Phil McMinn</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Used to establish confidence in the correctness of evolving software, regression testing is an important, yet costly, task. Test case prioritization enables the rapid detection of faults during regression testing by reordering the test suite so that effective tests are run as early as is possible. However, a distinct lack of information about the regression faults found in complex real-world software forced prior experimental studies of these methods to use artificial faults called mutants. Using the D<scp>efects</scp>4J database of real faults, this paper presents the results of experiments evaluating the effectiveness of four representative test prioritization techniques. Since this paper's results show that prioritization is susceptible to high amounts of variance when only one fault is present, our experiments also control the number of real faults and mutants in the program subject to regression testing. Our overall findings are that, in comparison to mutants, real faults are harder for reordered test suites to quickly detect, suggesting that mutants are not a surrogate for real faults.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652975&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=7zz158kK_aB2Vh_rbwrldV6sFK7IiVtGFPKOMu6gQpQ&s=kGEoH3QtVJfqSeEN6Sw_STx3Y0EgDUoX-Uhri2vq9s4&e=" title="Get the Full Text from the ACM Digital Library">Test suite reduction for self-organizing systems: a mutation-based approach</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Andr&#233; Reichstaller</li>
<li class="nameList">Benedikt Eberhardinger</li>
<li class="nameList">Hella Ponsar</li>
<li class="nameList">Alexander Knapp</li>
<li class="nameList Last">Wolfgang Reif</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>We study regression testing and test suite reduction for self-organizing (SO) systems. The complex environments of SO systems typically require large test suites. The physical distribution of their components and their history-dependent behavior, however, make test execution very expensive. Consequently, an efficient test suite reduction mechanism is needed. The fundamental characteristic of SO systems is their ability to reconfigure themselves. We thus investigate a mutation-based approach concentrating on reconfigurations, more specifically the communication between the distributed components in reconfigurations. Due to distribution, we argue for an explicit consideration of higher-order mutants and find a short-cut that makes the number of test cases to execute before reduction feasible. For the reduction task, we evaluate the applicability of two existing clustering techniques, <i>Affinity Propagation</i> and <i>Dissimilarity-based Sparse Subset Selection.</i> It turns out that these techniques are able to drastically reduce the original test suite while retaining a good mutation score. We discuss the approach by means of a test suite for a self-organizing production cell as a running example.</p></div> </div>
</div>
</div>
</body>
</html>
