<html xmlns:bkstg="http://www.atypon.com/backstage-ns" xmlns:urlutil="java:com.atypon.literatum.customization.UrlUtil" xmlns:pxje="java:com.atypon.frontend.services.impl.PassportXslJavaExtentions">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <meta http-equiv="Content-Style-Type" content="text/css">
      <style type="text/css">
            #DLtoc {
            font: normal 12px/1.5em Arial, Helvetica, sans-serif;
            }

            #DLheader {
            }
            #DLheader h1 {
            font-size:16px;
            }

            #DLcontent {
            font-size:12px;
            }
            #DLcontent h2 {
            font-size:14px;
            margin-bottom:5px;
            }
            #DLcontent h3 {
            font-size:12px;
            padding-left:20px;
            margin-bottom:0px;
            }

            #DLcontent ul{
            margin-top:0px;
            margin-bottom:0px;
            }

            .DLauthors li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLauthors li:after{
            content:",";
            }
            .DLauthors li.nameList.Last:after{
            content:"";
            }

            .DLabstract {
            padding-left:40px;
            padding-right:20px;
            display:block;
            }

            .DLformats li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLformats li:after{
            content:",";
            }
            .DLformats li.formatList.Last:after{
            content:"";
            }

            .DLlogo {
            vertical-align:middle;
            padding-right:5px;
            border:none;
            }

            .DLcitLink {
            margin-left:20px;
            }

            .DLtitleLink {
            margin-left:20px;
            }

            .DLotherLink {
            margin-left:0px;
            }

        </style>
      <title>MaLTESQuE 2021: Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution</title>
   </head>
   <body>
      <div id="DLtoc">
         <div id="DLheader">
            <h1>MaLTESQuE 2021: Proceedings of the 5th International Workshop on Machine Learning Techniques for Software
               Quality Evolution</h1><a class="DLcitLink" title="Go to the ACM Digital Library for additional information about this proceeding" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/proceedings/10.1145/3472674"><img class="DLlogo" alt="Digital Library logo" height="30" src="https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1.png">
               Full Citation in the ACM Digital Library
               </a></div>
         <div id="DLcontent">
            <h2>SESSION: Papers</h2>
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3472674.3473978">Comparing within- and cross-project machine learning algorithms for code smell detection</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Manuel De Stefano</li>
               <li class="nameList">Fabiano Pecorelli</li>
               <li class="nameList">Fabio Palomba</li>
               <li class="nameList Last">Andrea De Lucia</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Code smells represent a well-known problem in software engineering, since they are
                     a notorious cause of loss of comprehensibility and maintainability. The most recent
                     efforts in devising automatic machine learning-based code smell detection techniques
                     have achieved unsatisfying results so far. This could be explained by the fact that
                     all these approaches follow a within-project classification, i.e. training and test
                     data are taken from the same source project, which combined with the imbalanced nature
                     of the problem, produces datasets with a very low number of instances belonging to
                     the minority class (i.e. smelly instances). In this paper, we propose a cross-project
                     machine learning approach and compare its performance with a within-project alternative.
                     The core idea is to use transfer learning to increase the overall number of smelly
                     instances in the training datasets. Our results have shown that cross-project classification
                     provides very similar performance with respect to within-project. Despite this finding
                     does not yet provide a step forward in increasing the performance of ML techniques
                     for code smell detection, it sets the basis for further investigations.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3472674.3473979">Unsupervised learning of general-purpose embeddings for code changes</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Mikhail Pravilov</li>
               <li class="nameList">Egor Bogomolov</li>
               <li class="nameList">Yaroslav Golubev</li>
               <li class="nameList Last">Timofey Bryksin</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Applying machine learning to tasks that operate with code changes requires their numerical
                     representation. In this work, we propose an approach for obtaining such representations
                     during pre-training and evaluate them on two different downstream tasks — applying
                     changes to code and commit message generation. During pre-training, the model learns
                     to apply the given code change in a correct way. This task requires only code changes
                     themselves, which makes it unsupervised. In the task of applying code changes, our
                     model outperforms baseline models by 5.9 percentage points in accuracy. As for the
                     commit message generation, our model demonstrated the same results as supervised models
                     trained for this specific task, which indicates that it can encode code changes well
                     and can be improved in the future by pre-training on a larger dataset of easily gathered
                     code changes.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3472674.3473980">VaryMinions: leveraging RNNs to identify variants in event logs</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Sophie Fortz</li>
               <li class="nameList">Paul Temple</li>
               <li class="nameList">Xavier Devroey</li>
               <li class="nameList">Patrick Heymans</li>
               <li class="nameList Last">Gilles Perrouin</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Business processes have to manage <em>variability</em> in their execution, e.g., to deliver the correct building permit in different municipalities.
                     This variability is visible in event logs, where sequences of events are shared by
                     the core process (building permit authorisation) but may also be specific to each
                     municipality. To rationalise resources (e.g., derive a configurable business process
                     capturing all municipalities’ permit variants) or to debug anomalous behaviour, it
                     is mandatory to identify to which variant a given trace belongs. This paper supports
                     this task by training Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs)
                     algorithms on two datasets: a configurable municipality and a travel expenses workflow.
                     We demonstrate that variability can be identified accurately (&gt;87%) and discuss the
                     challenges of learning highly entangled variants.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3472674.3473981">Toward static test flakiness prediction: a feasibility study</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Valeria Pontillo</li>
               <li class="nameList">Fabio Palomba</li>
               <li class="nameList Last">Filomena Ferrucci</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Flaky tests are tests that exhibit both a passing and failing behavior when run against
                     the same code. While the research community has attempted to define automated approaches
                     for detecting and addressing test flakiness, most of them suffer from scalability
                     issues and uncertainty as they require test cases to be run multiple times. This limitation
                     has been recently targeted by means of machine learning solutions that could predict
                     the flakiness of tests using a set of both static and dynamic metrics that would avoid
                     the re-execution of tests. Recognizing the effort spent so far, this paper poses the
                     first steps toward an orthogonal view of the problem, namely the classification of
                     flaky tests using <em>only</em> statically computable software metrics. We propose a <em>feasibility study</em> on 72 projects of the iDFlakies dataset, and investigate the differences between
                     flaky and non-flaky tests in terms of 25 test and production code metrics and smells.
                     First, we statistically assess those differences. Second, we build a logistic regression
                     model to verify the extent to which the differences observed are still significant
                     when the metrics are considered together. The results show a relation between test
                     flakiness and a number of test and production code factors, indicating the possibility
                     to build classification approaches that exploit those factors to predict test flakiness.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3472674.3473982">Building a bot for automatic expert retrieval on discord</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Ignacio Nuñez Norambuena</li>
               <li class="nameList Last">Alexandre Bergel</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>It is common for software practitioners to look for experts on online chat platforms,
                     such as Discord. However, finding them is a complex activity that requires a deep
                     knowledge of the open source community. As a consequence, newcomers and casual participants
                     may not be able to adequately find experts willing to discuss a particular topic.
                     </p> 
                  <p> Our paper describes a bot that provides a ranked list of Discord users that are experts
                     in a particular set of topics. Our bot uses simple heuristics to model expertise,
                     such as a word occurrence table and word embeddings. Our bot shows that at least half
                     of the retrieved users are indeed experts.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3472674.3473983">Metrics selection for load monitoring of service-oriented system</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Francesco Lomio</li>
               <li class="nameList">Sampsa Jurvansuu</li>
               <li class="nameList Last">Davide Taibi</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p><em>Background.</em> Complex software systems produce a large amount of data depicting their internal
                     state and activities. The data can be monitored to make estimations and predictions
                     of the status of the system, helping taking preventative actions in case of impending
                     malfunctions and failures. However, a complex system may reveal thousands of internal
                     metrics, which makes it a non-trivial task to decide which metrics are the most important
                     to monitor. </p> 
                  <p><em>Objective.</em> In this work we aim at finding a subset of metrics to collect and analyse for the
                     monitoring of the load in a Service-oriented system. </p> 
                  <p><em>Method.</em> We use a performance test bench tool to generate load of different intensities on
                     the target system, which is a specific service-oriented application platform. The
                     numeric metrics data collected from the system is combined with the load intensity
                     at each moment. The combined data is used to analyse which metrics are best at estimating
                     the load of the system. By using a regression analysis it was possible to rank the
                     metrics by their ability to measure the load of the system. </p> 
                  <p><em>Results.</em> The results show that (1) the use of machine learning regressor allows to correctly
                     measure the load of a service-oriented system, and (2) the most important metrics
                     are related to network traffic and request counts, as well as memory usage and disk
                     activity. </p> 
                  <p><em>Conclusion.</em> The results help with the designs of efficient monitoring tool. In addition, further
                     investigation should be focused on exploring more precise machine learning model to
                     further improve the metric selection process.</p>
                  	</div>
            </div>
            						
            					</div>
      </div>
   </body>
</html>