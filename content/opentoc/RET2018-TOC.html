
<!doctype html>
<head>
<META http-equiv="Content-Style-Type" content="text/css">
<title>RET '18- Proceedings of the 5th International Workshop on Requirements Engineering and Testing</title>
<STYLE type="text/css">
#DLtoc {
	font: normal 12px/1.5em Arial, Helvetica, sans-serif;
	}

#DLheader {
	}
#DLheader h1 {
	font-size:16px;	
}
	
#DLcontent {
	 font-size:12px;
	}
#DLcontent h2 {
	 font-size:14px;
	 margin-bottom:5px;
	}
#DLcontent h3 {
	 font-size:12px;
	 padding-left:20px;
	 margin-bottom:0px;
	}

#DLcontent ul{
	margin-top:0px;
	margin-bottom:0px;
	}
		
.DLauthors li{
	display: inline;
	list-style-type: none;
	padding-right: 5px;
	}
	
.DLauthors li:after{
	content:",";
	}
.DLauthors li.nameList.Last:after{
	content:"";
	}		

.DLabstract {
	 padding-left:40px;
	 padding-right:20px;
	 display:block;
	}

.DLformats li{
	display: inline;
	list-style-type: none;
	padding-right: 5px;
	}
	
.DLformats li:after{
	content:",";
	}
.DLformats li.formatList.Last:after{
	content:"";
	}		

.DLlogo {
	vertical-align:middle; 
	padding-right:5px;
	border:none;
	}
	
.DLcitLink {
	margin-left:20px;
	}	

.DLtitleLink {
	margin-left:20px;
	}	

.DLotherLink {
	margin-left:0px;
	}		
   
</STYLE>
</head>
<body>
<div id="DLtoc">
<div id="DLheader">
<h1>RET '18- Proceedings of the 5th International Workshop on Requirements Engineering and Testing</h1>
<a class="DLcitLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_citation.cfm-3Fid-3D3195538&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=niS6Jm1TLQreD3j3SWUqKAX90OrsTm3SbDmRvmiLwgs&e=" title="Go to the ACM Digital Library for additional information about this proceeding"><img class="DLlogo" src="https://dl.acm.org/img/dllogo.png" alt="Digital Library logo" height="30" width="30">Full Citation in the ACM Digital Library</a>
</div>
<div id="DLcontent">
<h2>SESSION: Testing efficiency</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652265&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=YTzELk152EbUl3zky-F4BTQKAFYDqalSEohcmenG71I&e=" title="Get the Full Text from the ACM Digital Library">Cluster-based test scheduling strategies using semantic relationships between test specifications</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Sahar Tahvili</li>
<li class="nameList">Leo Hatvani</li>
<li class="nameList">Michael Felderer</li>
<li class="nameList">Wasif Afzal</li>
<li class="nameList">Mehrdad Saadatmand</li>
<li class="nameList Last">Markus Bohlin</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>One of the challenging issues in improving the test efficiency is that of achieving a balance between testing goals and testing resources. Test execution scheduling is one way of saving time and budget, where a set of test cases are grouped and tested at the same time. To have an optimal test execution schedule, all related information of a test case (e.g. execution time, functionality to be tested, dependency and similarity with other test cases) need to be analyzed. Test scheduling problem becomes more complicated at high-level testing, such as integration testing and especially in manual testing procedure. Test specifications are generally written in natural text by humans and usually contain ambiguity and uncertainty. Therefore, analyzing a test specification demands a strong learning algorithm. In this position paper, we propose a natural language processing-based approach that, given test specifications at the integration level, allows automatic detection of test cases semantic dependencies. The proposed approach utilizes the Doc2Vec algorithm and converts each test case into a vector in n-dimensional space. These vectors are then grouped using the HDBSCAN clustering algorithm into semantic clusters. Finally, a set of cluster-based test scheduling strategies are proposed for execution. The proposed approach has been applied in a sub-system from the railway domain by analyzing an ongoing testing project at Bombardier Transportation AB, Sweden.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652266&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=MIyfuai-CACcT6fIBBTLRtQ4Yi1JHBaWaeV7DGGzPxs&e=" title="Get the Full Text from the ACM Digital Library">Automated test-design from requirements: the Specmate tool</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Dietmar Freudenstein</li>
<li class="nameList">Jeannette Radduenz</li>
<li class="nameList">Maximilian Junker</li>
<li class="nameList">Sebastian Eder</li>
<li class="nameList Last">Benedikt Hauptmann</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Designing a small set of tests that nonetheless cover the requirements sufficiently is tantamount to keep costs for testing at bay while still maintaining the necessary quality. Engineering an optimal test-suite requires both, insight into the domain and the system under test, but also carefully examining the combinatorics inherent in the requirements. Especially the second part is a cognitive challenge and systematic methods are cumbersome when performed by hand. In this paper, we present Specmate, a tool that supports and partly automates the design of tests from requirements. It provides light-weight modeling techniques to capture requirements, test generation facilities to create test specifications and further supporting functions to derive test procedures or test-scripts from specifications. Specmate has been developed and evaluated in the context of one of the core business systems of Allianz Deutschland, a large insurance company. The source code is freely available at GitHub and an online-demo of Specmate is available at http://specmate.in.tum.de.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652267&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=qW9Fp5A8u47asGgUN5vrFhFFngV2sr-Wghv5tszjFTE&e=" title="Get the Full Text from the ACM Digital Library">Test case quality as perceived in Sweden</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Anders Adlemo</li>
<li class="nameList">He Tan</li>
<li class="nameList Last">Vladimir Tarasov</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>In order to reach an acceptable level of confidence in the quality of a software product, testing of the software is paramount. To obtain "good" quality software it is essential to rely on "good" test cases. To define the criteria for what make up for a "good" test case is not a trivial task. Over the past 15 years, a short list of publications have presented criteria for "good" test cases but without ranking them based on their importance. This paper presents a non-exhaustive and non-authoritative tentative list of 15 criteria and a ranking of their relative importance. A number of the criteria come from previous publications but also from discussions with our industrial partners. The ranking is based on results collected via a questionnaire that was sent out to a limited number of randomly chosen respondents in the Swedish software industry. This means that the results are more indicative than conclusive.</p></div> </div>
<h2>SESSION: Blurring the lines between requirements engineering and testing</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652268&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=AKXOlcVdYBjORnXPe2lUzViBq8mDSIzuIfCF4gZK-No&e=" title="Get the Full Text from the ACM Digital Library">A case study of interactive development of passive tests</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Daniel Flemstr&#246;m</li>
<li class="nameList">Thomas Gustafsson</li>
<li class="nameList Last">Avenir Kobetski</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Testing in the active sense is the most common way to perform verification and validation of systems, but testing in the passive sense has one compelling property: independence. Independence from test stimuli and other passive tests opens up for parallel testing and off-line analysis. However, the tests can be difficult to develop since the complete testable state must be expressed using some formalism. We argue that a carefully chosen language together with an interactive work flow, providing immediate feedback, can enable testers to approach passive testing. We have conducted a case study in the automotive domain, interviewing experienced testers. The testers have been introduced to, and had hands-on practice with a tool. The tool is based on Easy Approach to Requirements Syntax (EARS) and provides an interactive work flow for developing and evaluating test results. The case study shows that i) the testers believe passive testing is useful for many of their tests, ii) they see benefits in parallelism and off-line analysis, iii) the interactive work flow is necessary for writing the testable state expression, but iv) when the testable state becomes too complex, then the proposed language is a limitation. However, the language contributes to concise tests, resembling executable requirements.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652269&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=sj5q7K2i-JP_cOPh9Pqcifq0jFRrru9phTDp_RW7dac&e=" title="Get the Full Text from the ACM Digital Library">Towards a functional requirements prioritization with early mutation testing</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Nelly Condori-Fernandez</li>
<li class="nameList">Maria Fernanda Granda</li>
<li class="nameList Last">Tanja E. J. Vos</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Researchers have proposed a number of prioritization techniques to help decision makers select an optimal combination of (non-) functional requirements to implement. However, most of them are defined based on an ordinal or nominal scale, which are not reliable because they are limited to simple operations of ranked or ordered requirements. We argue that the importance of certain requirements could be determined by their criticality level, which can be assessed using a ratio scale.</p> <p>The main contribution of the paper is the new strategy proposed for prioritizing functional requirements, using early mutation testing and dependency analysis.</p></div> </div>
<h2>SESSION: Requirements quality impact on testing</h2>
<div class="DLabstract"> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652260&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=bUw7IXZMEDkRyY9fL82E-roNT5sjANjVxSdqawfEXXc&e=" title="Get the Full Text from the ACM Digital Library">Measuring and improving testability of system requirements in an industrial context by applying the goal question metric approach</a>
</h3>
 <ul class="DLauthors">
<li class="nameList First">Armin Beer</li>
<li class="nameList Last">Michael Felderer</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Testing is subject to two basic constraints, namely cost and quality. The cost depends on the efficiency of the testing activities as well as their quality and testability. The author's practical experience in large-scale systems shows that if the requirements are adapted iteratively or the architecture is altered, testability decreases. However, what is often lacking is a root cause analysis of the testability degradations and the introduction of improvement measures during software development. In order to introduce agile practices in the rigid strategy of the V-model, good testability of software artifacts is vital. So testability is also the bridgehead towards agility. In this paper, we report on a case study in which we measure and improve testability on the basis of the Goal Question Metric Approach.</p></div> </div>
<h3>
<a class="DLtitleLink" href="https://urldefense.proofpoint.com/v2/url?u=https-3A__dl.acm.org_authorize-3FN652261&d=DwMFAg&c=clK7kQUTWtAVEOVIgvi0NU5BOUHhpN0H8p7CSfnc_gI&r=ZHuLpaRqk3Uz8lrvKUHs4g&m=VG2z4fpFJp55kmP50Irc-PKpRH29b9o1spLEQN3hGYs&s=EMLsWqy4ZZ1qUmzXmfu0rEf5adaxhYLI57YdSfPlhgs&e=" title="Get the Full Text from the ACM Digital Library">Ambiguous software requirement specification detection: an automated approach</a>
</h3>
<ul class="DLauthors">
<li class="nameList First">Mohd Hafeez Osman</li>
<li class="nameList Last">Mohd Firdaus Zaharin</li>
</ul>
<div class="DLabstract"><div style="display:inline"><p>Software requirement specification (SRS) document is the most crucial document in software development process. All subsequent steps in software development are influenced by this document. However, issues in requirement, such as ambiguity or incomplete specification may lead to misinterpretation of requirements which consequently, influence the testing activities and higher the risk of time and cost overrun of the project. Finding defects in the initial development phase is crucial since the defect that found late is more expensive than if it was found early. This study describes an automated approach for detecting ambiguous software requirement specification. To this end, we propose the combination of text mining and machine learning. Since the dataset is derived from Malaysian industrial SRS documents, this study only focuses on the Malaysian context. We used text mining for feature extraction and for preparing the training set. Based on this training set, the method 'learns' to detect the ambiguous requirement specification.</p> <p>In this paper, we study a set of nine (9) classification algorithms from the machine learning community and evaluate which algorithm performs best to detect the ambiguous software requirement specification. Based on the experiment's result, we develop a working prototype which later is used for our initial validation of our approach. The initial validation shows that the result produced by the classification model is reasonably acceptable. Even though this study is an experimental benchmark, we optimist that this approach may contributes to enhance the quality of SRS.</p></div> </div>
</div>
</div>
</body>
</html>
