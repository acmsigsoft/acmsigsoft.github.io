<html xmlns:bkstg="http://www.atypon.com/backstage-ns" xmlns:urlutil="java:com.atypon.literatum.customization.UrlUtil" xmlns:pxje="java:com.atypon.frontend.services.impl.PassportXslJavaExtentions">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <meta http-equiv="Content-Style-Type" content="text/css">
      <style type="text/css">
            #DLtoc {
            font: normal 12px/1.5em Arial, Helvetica, sans-serif;
            }

            #DLheader {
            }
            #DLheader h1 {
            font-size:16px;
            }

            #DLcontent {
            font-size:12px;
            }
            #DLcontent h2 {
            font-size:14px;
            margin-bottom:5px;
            }
            #DLcontent h3 {
            font-size:12px;
            padding-left:20px;
            margin-bottom:0px;
            }

            #DLcontent ul{
            margin-top:0px;
            margin-bottom:0px;
            }

            .DLauthors li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLauthors li:after{
            content:",";
            }
            .DLauthors li.nameList.Last:after{
            content:"";
            }

            .DLabstract {
            padding-left:40px;
            padding-right:20px;
            display:block;
            }

            .DLformats li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLformats li:after{
            content:",";
            }
            .DLformats li.formatList.Last:after{
            content:"";
            }

            .DLlogo {
            vertical-align:middle;
            padding-right:5px;
            border:none;
            }

            .DLcitLink {
            margin-left:20px;
            }

            .DLtitleLink {
            margin-left:20px;
            }

            .DLotherLink {
            margin-left:0px;
            }

        </style>
      <title>AISTA 2021: Proceedings of the 1st ACM International Workshop on AI and Software Testing/Analysis</title>
   </head>
   <body>
      <div id="DLtoc">
         <div id="DLheader">
            <h1>AISTA 2021: Proceedings of the 1st ACM International Workshop on AI and Software Testing/Analysis</h1><a class="DLcitLink" title="Go to the ACM Digital Library for additional information about this proceeding" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/proceedings/10.1145/3464968"><img class="DLlogo" alt="Digital Library logo" height="30" src="https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1.png">
               Full Citation in the ACM Digital Library
               </a></div>
         <div id="DLcontent">
            <h2>SESSION: Papers</h2>
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3464968.3468407">On the use of evolutionary algorithms for test case prioritization in regression testing
                  considering requirements dependencies</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Andreea Vescan</li>
               <li class="nameList">Camelia Chisalita-Cretu</li>
               <li class="nameList">Camelia Serban</li>
               <li class="nameList Last">Laura Diosan</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Nowadays, software systems encounter repeated modifications in order to satisfy any
                     requirement regarding a business change. To assure that these changes do not affect
                     systems' proper functioning, those parts affected by the changes need to be retested,
                     minimizing the negative impact of performed modifications on another part of the software.
                     In this research, we investigate how different optimization techniques (with various
                     criteria) could improve the effectiveness of the testing activity, in particular the
                     effectiveness of test case prioritization. </p> 
                  <p> The most efficient test schedules are identified by using either a Greedy algorithm
                     or a Genetic Algorithm, optimizing a quality function that incorporates single or
                     multiple criteria. Both functional requirements (with the existing dependencies between
                     them) and non-functional requirements (i.e. quality attributes for test cases) are
                     integrated with the quality assessment of a test order. </p> 
                  <p> Therefore, the conducted experiments considered various criteria combinations (faults,
                     costs, and number of test cases), being applied to both theoretical case studies and
                     a real-world benchmark. The conclusion of the experiments shows that the Genetic Algorithm
                     outperforms the Greedy on all considered criteria.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3464968.3468408">Impact of programming languages on machine learning bugs</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Sebastian Sztwiertnia</li>
               <li class="nameList">Maximilian Gr√ºbel</li>
               <li class="nameList">Amine Chouchane</li>
               <li class="nameList">Daniel Sokolowski</li>
               <li class="nameList">Krishna Narasimhan</li>
               <li class="nameList Last">Mira Mezini</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Machine learning (ML) is on the rise to be ubiquitous in modern software. Still, its
                     use is challenging for software developers. So far, research has focused on the ML
                     libraries to find and mitigate these challenges. However, there is initial evidence
                     that programming languages also add to the challenges, identifiable in different distributions
                     of bugs in ML programs. To fill this research gap, we propose the first empirical
                     study on the impact of programming languages on bugs in ML programs. We plan to analyze
                     software from GitHub and related discussions in GitHub issues and Stack Overflow for
                     bug distributions in ML programs, aiming to identify correlations with the chosen
                     programming language, its features and the application domain. This study's results
                     enable better-targeted use of available programming language technology in ML programs,
                     preventing bugs, reducing errors and speeding up development.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3464968.3468409">NerdBug: automated bug detection in neural networks</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Foad Jafarinejad</li>
               <li class="nameList">Krishna Narasimhan</li>
               <li class="nameList Last">Mira Mezini</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Despite the exponential growth of deep learning software during the last decade, there
                     is a lack of tools to test and debug issues in deep learning programs. Current static
                     analysis tools do not address challenges specific to deep learning as observed by
                     past research on bugs specific to this area. Existing deep learning bug detection
                     tools focus on specific issues like shape mismatches. In this paper, we present a
                     vision for an abstraction-based approach to detect deep learning bugs and the plan
                     to evaluate our approach. The motivation behind the abstraction-based approach is
                     to be able to build an intermediate version of the neural network that can be analyzed
                     in development time to provide live feedback programmers are used to with other kind
                     of bugs.</p>
                  	</div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3464968.3468410">Automated cell header generator for Jupyter notebooks</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Ashwin Prasad Shivarpatna Venkatesh</li>
               <li class="nameList Last">Eric Bodden</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Jupyter notebooks are now widely adopted by data analysts as they provide a convenient
                     environment for presenting computational results in a literate-programming document
                     that combines code snippets, rich text, and inline visualizations. Literate-programming
                     documents are intended to be computational narratives that are supplemented with self-explanatory
                     text, but, recent studies have shown that this is lacking in practice. Efforts in
                     the software engineering community to increase code comprehension in literate programming
                     are limited. To address this, as a first step, this paper presents a prototype Jupyter
                     notebook annotator, HeaderGen, that automatically creates a narrative structure in
                     notebooks by classifying and annotating code cells based on the machine learning workflow.
                     HeaderGen generates a markdown cell header for each code cell by statically analyzing
                     the notebook, and in addition, associates these cell headers with a clickable table
                     of contents for easier navigation. Further, we discuss our vision and opportunities
                     based on this prototype.</p>
                  	</div>
            </div>
            						
            					</div>
      </div>
   </body>
</html>