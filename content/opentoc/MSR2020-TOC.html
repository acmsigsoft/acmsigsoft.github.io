<html xmlns:bkstg="http://www.atypon.com/backstage-ns" xmlns:urlutil="java:com.atypon.literatum.customization.UrlUtil" xmlns:pxje="java:com.atypon.frontend.services.impl.PassportXslJavaExtentions">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <meta http-equiv="Content-Style-Type" content="text/css"><style type="text/css">
            #DLtoc {
            font: normal 12px/1.5em Arial, Helvetica, sans-serif;
            }

            #DLheader {
            }
            #DLheader h1 {
            font-size:16px;
            }

            #DLcontent {
            font-size:12px;
            }
            #DLcontent h2 {
            font-size:14px;
            margin-bottom:5px;
            }
            #DLcontent h3 {
            font-size:12px;
            padding-left:20px;
            margin-bottom:0px;
            }

            #DLcontent ul{
            margin-top:0px;
            margin-bottom:0px;
            }

            .DLauthors li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLauthors li:after{
            content:",";
            }
            .DLauthors li.nameList.Last:after{
            content:"";
            }

            .DLabstract {
            padding-left:40px;
            padding-right:20px;
            display:block;
            }

            .DLformats li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLformats li:after{
            content:",";
            }
            .DLformats li.formatList.Last:after{
            content:"";
            }

            .DLlogo {
            vertical-align:middle;
            padding-right:5px;
            border:none;
            }

            .DLcitLink {
            margin-left:20px;
            }

            .DLtitleLink {
            margin-left:20px;
            }

            .DLotherLink {
            margin-left:0px;
            }

        </style><title>MSR '20: Proceedings of the 17th International Conference on Mining Software Repositories</title>
   </head>
   <body>
      <div id="DLtoc">
         <div id="DLheader">
            <h1>MSR '20: Proceedings of the 17th International Conference on Mining Software Repositories</h1><a class="DLcitLink" title="Go to the ACM Digital Library for additional information about this proceeding" href="https://dl.acm.org/doi/proceedings/10.1145/3379597"><img class="DLlogo" alt="Digital Library logo" height="30" src="https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1.png">
               Full Citation in the ACM Digital Library
               </a></div>
         <div id="DLcontent">
            <h2>SESSION: Mining Challenge</h2>
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387510">The Software Heritage Graph Dataset: Large-scale Analysis of Public Software Development History</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Antoine Pietri</li>
               <li class="nameList">Diomidis Spinellis</li>
               <li class="nameList Last">Stefano Zacchiroli</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Software Heritage is the largest existing public archive of software source code and
                     accompanying development history. It spans more than five billion unique source code
                     files and one billion unique commits, coming from more than 80 million software projects.
                     These software artifacts were retrieved from major collaborative development platforms
                     (e.g., GitHub, GitLab) and package repositories (e.g., PyPI, Debian, NPM), and stored
                     in a uniform representation linking together source code files, directories, commits,
                     and full snapshots of version control systems (VCS) repositories as observed by Software
                     Heritage during periodic crawls. This dataset is unique in terms of accessibility
                     and scale, and allows to explore a number of research questions on the long tail of
                     public software development, instead of solely focusing on "most starred" repositories
                     as it often happens.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387511">Cheating Death: A Statistical Survival Analysis of Publicly Available Python Projects</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Rao Hamza Ali</li>
               <li class="nameList">Chelsea Parlett-Pelleriti</li>
               <li class="nameList Last">Erik Linstead</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>We apply survival analysis methods to a dataset of publicly-available software projects
                     in order to examine the attributes that might lead to their inactivity over time.
                     We ran a Kaplan-Meier analysis and fit a Cox Proportional-Hazards model to a subset
                     of Software Heritage Graph Dataset, consisting of 3052 popular Python projects hosted
                     on GitLab/GitHub, Debian, and PyPI, over a period of 165 months. We show that projects
                     with repositories on multiple hosting services, a timeline of publishing major releases,
                     and a good network of developers, remain healthy over time and should be worthy of
                     the effort put in by developers and contributors.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387512">An Exploratory Study to Find Motives Behind Cross-platform Forks from Software Heritage
                  Dataset</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Avijit Bhattacharjee</li>
               <li class="nameList">Sristy Sumana Nath</li>
               <li class="nameList">Shurui Zhou</li>
               <li class="nameList">Debasish Chakroborti</li>
               <li class="nameList">Banani Roy</li>
               <li class="nameList">Chanchal K. Roy</li>
               <li class="nameList Last">Kevin Schneider</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The fork-based development mechanism provides the flexibility and the unified processes
                     for software teams to collaborate easily in a distributed setting without too much
                     coordination overhead. Currently, multiple social coding platforms support fork-based
                     development, such as GitHub, GitLab, and Bitbucket. Although these different platforms
                     virtually share the same features, they have different emphasis. As GitHub is the
                     most popular platform and the corresponding data is publicly available, most of the
                     current studies are focusing on GitHub hosted projects. However, we observed anecdote
                     evidences that people are confused about choosing among these platforms, and some
                     projects are migrating from one platform to another, and the reasons behind these
                     activities remain unknown. With the advances of Software Heritage Graph Dataset (SWHGD),
                     we have the opportunity to investigate the forking activities across platforms. In
                     this paper, we conduct an exploratory study on 10 popular open-source projects to
                     identify cross-platform forks and investigate the motivation behind. Preliminary result
                     shows that cross-platform forks do exist. For the 10 subject systems used in this
                     study, we found 81,357 forks in total among which 179 forks are on GitLab. Based on
                     our qualitative analysis, we found that most of the cross-platform forks that we identified
                     are mirrors of the repositories on another platform, but we still find cases that
                     were created due to preference of using certain functionalities (e.g. Continuous Integration
                     (CI)) supported by different platforms. This study lays the foundation of future research
                     directions, such as understanding the differences between platforms and supporting
                     cross-platform collaboration.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387513">Exploring the Security Awareness of the Python and JavaScript Open Source Communities</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Gábor Antal</li>
               <li class="nameList">Márton Keleti</li>
               <li class="nameList Last">Péter Hegedŭs</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Software security is undoubtedly a major concern in today's software engineering.
                     Although the level of awareness of security issues is often high, practical experiences
                     show that neither preventive actions nor reactions to possible issues are always addressed
                     properly in reality. By analyzing large quantities of commits in the open-source communities,
                     we can categorize the vulnerabilities mitigated by the developers and study their
                     distribution, resolution time, etc. to learn and improve security management processes
                     and practices.
                  </p> 
                  <p>With the help of the Software Heritage Graph Dataset, we investigated the commits
                     of two of the most popular script languages - Python and JavaScript - projects collected
                     from public repositories and identified those that mitigate a certain vulnerability
                     in the code (i.e. vulnerability resolution commits). On the one hand, we identified
                     the types of vulnerabilities (in terms of CWE groups) referred to in commit messages
                     and compared their numbers within the two communities. On the other hand, we examined
                     the average time elapsing between the publish date of a vulnerability and the first
                     reference to it in a commit.
                  </p> 
                  <p>We found that there is a large intersection in the vulnerability types mitigated by
                     the two communities, but most prevalent vulnerabilities are specific to language.
                     Moreover, neither the JavaScript nor the Python community reacts very fast to appearing
                     security vulnerabilities in general with only a couple of exceptions for certain CWE
                     groups.
                  </p>
                  	
               </div>
            </div>
            						
            					
            <h2>SESSION: Technical Papers</h2>
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387474">A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Shayan A. Akbar</li>
               <li class="nameList Last">Avinash C. Kak</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>This paper reports on a large-scale comparative evaluation of IR-based tools for automatic
                     bug localization. We have divided the tools in our evaluation into the following three
                     generations: (1) The first-generation tools, now over a decade old, that are based
                     purely on the Bag-of-Words (BoW) modeling of software libraries. (2) The somewhat
                     more recent second-generation tools that augment BoW-based modeling with two additional
                     pieces of information: historical data, such as change history, and structured information
                     such as class names, method names, etc. And, finally, (3) The third-generation tools
                     that are currently the focus of much research and that also exploit proximity, order,
                     and semantic relationships between the terms. It is important to realize that the
                     original authors of all these three generations of tools have mostly tested them on
                     relatively small-sized datasets that typically consisted no more than a few thousand
                     bug reports. Additionally, those evaluations only involved Java code libraries. The
                     goal of the present paper is to present a comprehensive large-scale evaluation of
                     all three generations of bug-localization tools with code libraries in multiple languages.
                     Our study involves over 20,000 bug reports drawn from a diverse collection of Java,
                     C/C++, and Python projects. Our results show that the third-generation tools are significantly
                     superior to the older tools. We also show that the word embeddings generated using
                     code files written in one language are effective for retrieval from code libraries
                     in other languages.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387461">A Machine Learning Approach for Vulnerability Curation</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Yang Chen</li>
               <li class="nameList">Andrew E. Santosa</li>
               <li class="nameList">Ang Ming Yi</li>
               <li class="nameList">Abhishek Sharma</li>
               <li class="nameList">Asankhaya Sharma</li>
               <li class="nameList Last">David Lo</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Software composition analysis depends on database of open-source library vulerabilities,
                     curated by security researchers using various sources, such as bug tracking systems,
                     commits, and mailing lists. We report the design and implementation of a machine learning
                     system to help the curation by by automatically predicting the vulnerability-relatedness
                     of each data item. It supports a complete pipeline from data collection, model training
                     and prediction, to the validation of new models before deployment. It is executed
                     iteratively to generate better models as new input data become available. We use self-training
                     to significantly and automatically increase the size of the training dataset, opportunistically
                     maximizing the improvement in the models' quality at each iteration. We devised new
                     deployment stability metric to evaluate the quality of the new models before deployment
                     into production, which helped to discover an error. We experimentally evaluate the
                     improvement in the performance of the models in one iteration, with 27.59% maximum
                     PR AUC improvements. Ours is the first of such study across a variety of data sources.
                     We discover that the addition of the features of the corresponding commits to the
                     features of issues/pull requests improve the precision for the recall values that
                     matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC
                     improvement, and we discover that there is no uniform ordering of word2vec parameters
                     sensitivity across data sources.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387470">A Soft Alignment Model for Bug Deduplication</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Irving Muller Rodrigues</li>
               <li class="nameList">Daniel Aloise</li>
               <li class="nameList">Eraldo Rezende Fernandes</li>
               <li class="nameList Last">Michel Dagenais</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Bug tracking systems (BTS) are widely used in software projects. An important task
                     in such systems consists of identifying duplicate bug reports, i.e., distinct reports
                     related to the same software issue. For several reasons, reporting bugs that have
                     already been reported is quite frequent, making their manual triage impractical in
                     large BTSs. In this paper, we present a novel deep learning network based on soft-attention
                     alignment to improve duplicate bug report detection. For a given pair of possibly
                     duplicate reports, the attention mechanism computes interdependent representations
                     for each report, which is more powerful than previous approaches. We evaluate our
                     model on four well-known datasets derived from BTSs of four popular open-source projects.
                     Our evaluation is based on a ranking-based metric, which is more realistic than decision-making
                     metrics used in many previous works. Achieved results demonstrate that our model outperforms
                     state-of-the-art systems and strong baselines in different scenarios. Finally, an
                     ablation study is performed to confirm that the proposed architecture improves the
                     duplicate bug reports detection.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387455">A Study of Potential Code Borrowing and License Violations in Java Projects on GitHub</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Yaroslav Golubev</li>
               <li class="nameList">Maria Eliseeva</li>
               <li class="nameList">Nikita Povarov</li>
               <li class="nameList Last">Timofey Bryksin</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>With an ever-increasing amount of open-source software, the popularity of services
                     like GitHub that facilitate code reuse, and common misconceptions about the licensing
                     of open-source software, the problem of license violations in the code is getting
                     more and more prominent. In this study, we compile an extensive corpus of popular
                     Java projects from GitHub, search it for code clones, and perform an original analysis
                     of possible code borrowing and license violations on the level of code fragments.
                     We chose Java as a language because of its popularity in industry, where the plagiarism
                     problem is especially relevant because of possible legal action. We analyze and discuss
                     distribution of 94 different discovered and manually evaluated licenses in files and
                     projects, differences in the licensing of files, distribution of potential code borrowing
                     between licenses, various types of possible license violations, most violated licenses,
                     etc. Studying possible license violations in specific blocks of code, we have discovered
                     that 29.6% of them might be involved in potential code borrowing and 9.4% of them
                     could potentially violate original licenses.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387468">A Study on the Accuracy of OCR Engines for Source Code Transcription from Programming
                  Screencasts</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Abdulkarim Khormi</li>
               <li class="nameList">Mohammad Alahmadi</li>
               <li class="nameList Last">Sonia Haiduc</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Programming screencasts can be a rich source of documentation for developers. However,
                     despite the availability of such videos, the information available in them, and especially
                     the source code being displayed is not easy to find, search, or reuse by programmers.
                     Recent work has identified this challenge and proposed solutions that identify and
                     extract source code from video tutorials in order to make it readily available to
                     developers or other tools. A crucial component in these approaches is the Optical
                     Character Recognition (OCR) engine used to transcribe the source code shown on screen.
                     Previous work has simply chosen one OCR engine, without consideration for its accuracy
                     or that of other engines on source code recognition. In this paper, we present an
                     empirical study on the accuracy of six OCR engines for the extraction of source code
                     from screencasts and code images. Our results show that the transcription accuracy
                     varies greatly from one OCR engine to another and that the most widely chosen OCR
                     engine in previous studies is by far not the best choice. We also show how other factors,
                     such as font type and size can impact the results of some of the engines. We conclude
                     by offering guidelines for programming screencast creators on which fonts to use to
                     enable a better OCR recognition of their source code, as well as advice on OCR choice
                     for researchers aiming to analyze source code in screencasts.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387483">An Empirical Study of Build Failures in the Docker Context</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Yiwen Wu</li>
               <li class="nameList">Yang Zhang</li>
               <li class="nameList">Tao Wang</li>
               <li class="nameList Last">Huaimin Wang</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Docker containers have become the de-facto industry standard. Docker builds often
                     break, and a large amount of efforts are put into troubleshooting broken builds. Prior
                     studies have evaluated the rate at which builds in large organizations fail. However,
                     little is known about the frequency and fix effort of failures that occur in Docker
                     builds of open-source projects. This paper provides a first attempt to present a preliminary
                     study on 857,086 Docker builds from 3,828 open-source projects hosted on GitHub. Using
                     the Docker build data, we measure the frequency of broken builds and report their
                     fix time. Furthermore, we explore the evolution of Docker build failures across time.
                     Our findings help to characterize and understand Docker build failures and motivate
                     the need for collecting more empirical evidence.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387448">AIMMX: Artificial Intelligence Model Metadata Extractor</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Jason Tsay</li>
               <li class="nameList">Alan Braz</li>
               <li class="nameList">Martin Hirzel</li>
               <li class="nameList">Avraham Shinnar</li>
               <li class="nameList Last">Todd Mummert</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Despite all of the power that machine learning and artificial intelligence (AI) models
                     bring to applications, much of AI development is currently a fairly ad hoc process.
                     Software engineering and AI development share many of the same languages and tools,
                     but AI development as an engineering practice is still in early stages. Mining software
                     repositories of AI models enables insight into the current state of AI development.
                     However, much of the relevant metadata around models are not easily extractable directly
                     from repositories and require deduction or domain knowledge. This paper presents a
                     library called AIMMX that enables simplified AI Model Metadata eXtraction from software
                     repositories. The extractors have five modules for extracting AI model-specific metadata:
                     model name, associated datasets, references, AI frameworks used, and model domain.
                     We evaluated AIMMX against 7,998 open-source models from three sources: model zoos,
                     arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with
                     87% precision and 83% recall. As preliminary examples of how AI model metadata extraction
                     enables studies and tools to advance engineering support for AI development, this
                     paper presents an exploratory analysis for data and method reproducibility over the
                     models in the evaluation dataset and a catalog tool for discovering and managing models.
                     Our analysis suggests that while data reproducibility may be relatively poor with
                     42% of models in our sample citing their datasets, method reproducibility is more
                     common at 72% of models in our sample, particularly state-of-the-art models. Our collected
                     models are searchable in a catalog that uses existing metadata to enable advanced
                     discovery features for efficiently finding models.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387441">An Empirical Study of Method Chaining in Java</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Tomoki Nakamaru</li>
               <li class="nameList">Tomomasa Matsunaga</li>
               <li class="nameList">Tetsuro Yamazaki</li>
               <li class="nameList">Soramichi Akiyama</li>
               <li class="nameList Last">Shigeru Chiba</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>While some promote method chaining as a good practice for improving code readability,
                     others refer to it as a bad practice that worsens code quality. In this paper, we
                     first investigate whether method chaining is a programming style accepted by real-world
                     programmers. To answer this question, we collected 2,814 Java repositories on GitHub
                     and analyzed historical trends in the frequency of method chaining. The results of
                     our analysis revealed the increasing use of method chaining; 23.1% of method invocations
                     were part of method chains in 2018, whereas only 16.0% were such invocations in 2010.
                     We then explore language features that are helpful to the method-chaining style but
                     have not been supported yet in Java. For this aim, we conducted manual inspections
                     of method chains that are randomly sampled from the collected repositories. We also
                     estimated how effective they are to encourage the method-chaining style if they are
                     adopted in Java.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387464">An Empirical Study on Regular Expression Bugs</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Peipei Wang</li>
               <li class="nameList">Chris Brown</li>
               <li class="nameList">Jamie A. Jennings</li>
               <li class="nameList Last">Kathryn T. Stolee</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Understanding the nature of regular expression (regex) issues is important to tackle
                     practical issues developers face in regular expression usage. Knowledge about the
                     nature and frequency of various types of regular expression issues, such as those
                     related to performance, API misuse, and code smells, can guide testing, inform documentation
                     writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression
                     Denial of Service), little is known about to what extent regular expression issues
                     affect software development and how these issues are addressed in practice.
                  </p> 
                  <p>This paper presents a comprehensive empirical study of 350 merged regex-related pull
                     requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying
                     the root causes and manifestations of those bugs, we show that incorrect regular expression
                     behavior is the dominant root cause of regular expression bugs (165/356, 46.3%). The
                     remaining root causes are incorrect API usage (9.3%) and other code issues that require
                     regular expression changes in the fix (29.5%). By studying the code changes of regex-related
                     pull requests, we observe that fixing regular expression bugs is nontrivial as it
                     takes more time and more lines of code to fix them compared to the general pull requests.
                     The results of this study contribute to a broader understanding of the practical problems
                     faced by developers when using regular expressions.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387469">Automatically Granted Permissions in Android apps: An Empirical Study on their Prevalence and on the Potential Threats for Privacy</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Paolo Calciati</li>
               <li class="nameList">Konstantin Kuznetsov</li>
               <li class="nameList">Alessandra Gorla</li>
               <li class="nameList Last">Andreas Zeller</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Developers continuously update their Android apps to keep up with competitors in the
                     market. Such constant updates do not bother end users, since by default the Android
                     platform automatically pushes the most recent compatible release on the device, unless
                     there are major changes in the list of requested permissions that users have to explicitly
                     grant. The lack of explicit user's approval for each application update, however,
                     may lead to significant risks for the end user, as the new release may include new
                     subtle behaviors which may be privacy-invasive. The introduction of permission groups
                     in the Android permission model makes this problem even worse: if a user gives a single
                     permission within a group, the application can silently request further permissions
                     in this group with each update---without having to ask the user.
                  </p> 
                  <p>In this paper, we explain the threat that permission groups may pose for the privacy
                     of Android users. We run an empirical study on 2,865,553 app releases, and we show
                     that in a representative app store more than ~17% of apps request at least once in
                     their lifetime new dangerous permissions that the operating system grants without
                     any user's approval. Our analyses show that apps actually use over 56% of such automatically
                     granted permissions, although most of their descriptions do not explicitly explain
                     for what purposes. Finally, our manual inspection reveals clear abuses of apps that
                     leak sensitive data such as user's accurate location, list of contacts, history of
                     phone calls, and emails which are protected by permissions that the user never explicitly
                     acknowledges.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387475">Behind the Intents: An In-depth Empirical Study on Software Refactoring in Modern Code Review</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Matheus Paixão</li>
               <li class="nameList">Anderson Uchôa</li>
               <li class="nameList">Ana Carla Bibiano</li>
               <li class="nameList">Daniel Oliveira</li>
               <li class="nameList">Alessandro Garcia</li>
               <li class="nameList">Jens Krinke</li>
               <li class="nameList Last">Emilio Arvonio</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Code refactorings are of pivotal importance in modern code review. Developers may
                     preserve, revisit, add or undo refactorings through changes' revisions. Their goal
                     is to certify that the driving intent of a code change is properly achieved. Developers'
                     intents behind refactorings may vary from pure structural improvement to facilitating
                     feature additions and bug fixes. However, there is little understanding of the refactoring
                     practices performed by developers during the code review process. It is also unclear
                     whether the developers' intents influence the selection, composition, and evolution
                     of refactorings during the review of a code change. Through mining 1,780 reviewed
                     code changes from 6 systems pertaining to two large open-source communities, we report
                     the first in-depth empirical study on software refactoring during code review. We
                     inspected and classified the developers' intents behind each code change into 7 distinct
                     categories. By analyzing data generated during the complete reviewing process, we
                     observe: (i) how refactorings are selected, composed and evolved throughout each code
                     change, and (ii) how developers' intents are related to these decisions. For instance,
                     our analysis shows developers regularly apply non-trivial sequences of refactorings
                     that crosscut multiple code elements (i.e., widely scattered in the program) to support
                     a single feature addition. Moreover, we observed that new developers' intents commonly
                     emerge during the code review process, influencing how developers select and compose
                     their refactorings to achieve the new and adapted goals. Finally, we provide an enriched
                     dataset that allows researchers to investigate the context and motivations behind
                     refactoring operations during the code review process.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387459">Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Laerte Xavier</li>
               <li class="nameList">Fabio Ferreira</li>
               <li class="nameList">Rodrigo Brito</li>
               <li class="nameList Last">Marco Tulio Valente</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Self-admitted technical debt (SATD) is a particular case of Technical Debt (TD) where
                     developers explicitly acknowledge their sub-optimal implementation decisions. Previous
                     studies mine SATD by searching for specific TD-related terms in source code comments.
                     By contrast, in this paper we argue that developers can admit technical debt by other
                     means, e.g., by creating issues in tracking systems and labelling them as referring
                     to TD. We refer to this type of SATD as issue-based SATD or just SATD-I. We study
                     a sample of 286 SATD-I instances collected from five open source projects, including
                     Microsoft Visual Studio and GitLab Community Edition. We show that only 29% of the
                     studied SATD-I instances can be tracked to source code comments. We also show that
                     SATD-I issues take more time to be closed, compared to other issues, although they
                     are not more complex in terms of code churn. Besides, in 45% of the studied issues
                     TD was introduced to ship earlier, and in almost 60% it refers to DESIGN flaws. Finally,
                     we report that most developers pay SATD-I to reduce its costs or interests (66%).
                     Our findings suggest that there is space for designing novel tools to support technical
                     debt management, particularly tools that encourage developers to create and label
                     issues containing TD concerns.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387480">Boa Views: Easy Modularization and Sharing of MSR Analyses</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Che Shian Hung</li>
               <li class="nameList Last">Robert Dyer</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Mining Software Repositories (MSR) has recently seen a focus toward ultra-large-scale
                     datasets. Several tools exist to support these efforts, such as the Boa language and
                     infrastructure. While Boa has seen extensive use, in its current form it is not always
                     possible to perform the entire analysis task within the infrastructure, often requiring
                     some post-processing in another language. This limits end-to-end reproducibility and
                     limits sharing/re-use of MSR queries. To address this problem, we use the notion of
                     views from the relational database field and designed a query language and runtime
                     infrastructure extension for Boa that we call materialized views. Materialized views
                     provide output reuse to Boa users, so that the results of prior Boa queries can be
                     easily reused by others. This allows for computing results not previously possible
                     within Boa and provides for increased sharing and reuse of MSR queries. To evaluate
                     views, we performed two partial reproductions of prior MSR studies utilizing Boa's
                     dataset and infrastructure and compare our results to the prior studies. This shows
                     the usability of the new infrastructure, allowing analyses in Boa that were not previously
                     possible as well as providing a previously hand created gold dataset for identifier
                     splitting as a reusable view for other MSR researchers. We also verified the caching
                     behavior using the queries from one of the case studies. The results show that caching
                     works as expected and can drastically improve runtime performance.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387446">Can We Use SE-specific Sentiment Analysis Tools in a Cross-Platform Setting?</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Nicole Novielli</li>
               <li class="nameList">Fabio Calefato</li>
               <li class="nameList">Davide Dongiovanni</li>
               <li class="nameList">Daniela Girardi</li>
               <li class="nameList Last">Filippo Lanubile</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>In this paper, we address the problem of using sentiment analysis tools 'off-the-shelf',
                     that is when a gold standard is not available for retraining. We evaluate the performance
                     of four SE-specific tools in a cross-platform setting, i.e., on a test set collected
                     from data sources different from the one used for training. We find that (i) the lexicon-based
                     tools outperform the supervised approaches retrained in a cross-platform setting and
                     (ii) retraining can be beneficial in within-platform settings in the presence of robust
                     gold standard datasets, even using a minimal training set. Based on our empirical
                     findings, we derive guidelines for reliable use of sentiment analysis tools in software
                     engineering.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387463">Capture the Feature Flag: Detecting Feature Flags in Open-Source</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Jens Meinicke</li>
               <li class="nameList">Juan Hoyos</li>
               <li class="nameList">Bogdan Vasilescu</li>
               <li class="nameList Last">Christian Kästner</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Feature flags (a.k.a feature toggles) are a mechanism to keep new features hidden
                     behind a boolean option during development. Flags are used for many purposes, such
                     as A/B testing and turning off a feature more easily in case of failures. While software
                     engineering research on feature flags is burgeoning, examples of software projects
                     using flags rarely come from outside commercial and private projects, stifling academic
                     progress. To address this gap, in this paper we present a novel semi-automated mining
                     software repositories approach to detect feature flags in open-source projects, based
                     on analyzing the projects' commit messages and other project characteristics. With
                     our approach, we search over all open-source GitHub projects, finding multiple thousand
                     plausible and active candidate feature flagging projects. We manually validate projects
                     and assemble a dataset of 100 confirmed feature flagging projects. To demonstrate
                     the benefits of our detection technique, we report on an initial analysis of feature
                     flags in the validated sample of 100 projects, investigating practices that correlate
                     with shorter flag lifespans (typically desirable to reduce technical debt), such as
                     using the issue tracker and having a flag owner.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387472">Challenges in Chatbot Development: A Study of Stack Overflow Posts</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Ahmad Abdellatif</li>
               <li class="nameList">Diego Costa</li>
               <li class="nameList">Khaled Badran</li>
               <li class="nameList">Rabe Abdalkareem</li>
               <li class="nameList Last">Emad Shihab</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Chatbots are becoming increasingly popular due to their benefits in saving costs,
                     time, and effort. This is due to the fact that they allow users to communicate and
                     control different services easily through natural language. Chatbot development requires
                     special expertise (e.g., machine learning and conversation design) that differ from
                     the development of traditional software systems. At the same time, the challenges
                     that chatbot developers face remain mostly unknown since most of the existing studies
                     focus on proposing chatbots to perform particular tasks rather than their development.
                  </p> 
                  <p>Therefore, in this paper, we examine the Q&amp;A website, Stack Overflow, to provide insights
                     on the topics that chatbot developers are interested and the challenges they face.
                     In particular, we leverage topic modeling to understand the topics that are being
                     discussed by chatbot developers on Stack Overflow. Then, we examine the popularity
                     and difficulty of those topics. Our results show that most of the chatbot developers
                     are using Stack Overflow to ask about implementation guidelines. We determine 12 topics
                     that developers discuss (e.g., Model Training) that fall into five main categories.
                     Most of the posts belong to chatbot development, integration, and the natural language
                     understanding (NLU) model categories. On the other hand, we find that developers consider
                     the posts of building and integrating chatbots topics more helpful compared to other
                     topics. Specifically, developers face challenges in the training of the chatbot's
                     model. We believe that our study guides future research to propose techniques and
                     tools to help the community at its early stages to overcome the most popular and difficult
                     topics that practitioners face when developing chatbots.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387477">Characterizing and Identifying Composite Refactorings: Concepts, Heuristics and Patterns</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Leonardo Sousa</li>
               <li class="nameList">Diego Cedrim</li>
               <li class="nameList">Alessandro Garcia</li>
               <li class="nameList">Willian Oizumi</li>
               <li class="nameList">Ana C. Bibiano</li>
               <li class="nameList">Daniel Oliveira</li>
               <li class="nameList">Miryung Kim</li>
               <li class="nameList Last">Anderson Oliveira</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Refactoring consists of a transformation applied to improve the program internal structure,
                     for instance, by contributing to remove code smells. Developers often apply multiple
                     interrelated refactorings called composite refactoring. Even though composite refactoring
                     is a common practice, an investigation from different points of view on how composite
                     refactoring manifests in practice is missing. Previous empirical studies also neglect
                     how different kinds of composite refactorings affect the removal, prevalence or introduction
                     of smells. To address these matters, we provide a conceptual framework and two heuristics
                     to respectively characterize and identify composite refactorings within and across
                     commits. Then, we mined the commit history of 48 GitHub software projects. We identified
                     and analyzed 24,911 composite refactorings involving 104,505 single refactorings.
                     Amongst several findings, we observed that most composite refactorings occur in the
                     same commit and have the same refactoring type. We found that several refactorings
                     are semantically related to each other, which occur in different parts of the system
                     but are still related to the same task. Our study is the first to reveal that many
                     smells are introduced in a program due to "incomplete" composite refactorings. Our
                     study is also the first to reveal 111 patterns of composite refactorings that frequently
                     introduce or remove certain smell types. These patterns can be used as guidelines
                     for developers to improve their refactoring practices as well as for designers of
                     recommender systems.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387454">Detecting Video Game-Specific Bad Smells in Unity Projects</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Antonio Borrelli</li>
               <li class="nameList">Vittoria Nardone</li>
               <li class="nameList">Giuseppe A. Di Lucca</li>
               <li class="nameList">Gerardo Canfora</li>
               <li class="nameList Last">Massimiliano Di Penta</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The growth of the video game market, the large proportion of games targeting mobile
                     devices or streaming services, and the increasing complexity of video games trigger
                     the availability of video game-specific tools to assess performance and maintainability
                     problems. This paper proposes UnityLinter, a static analysis tool that supports Unity
                     video game developers to detect seven types of bad smells we have identified as relevant
                     in video game development. Such smell types pertain to performance, maintainability
                     and incorrect behavior problems. After having defined the smells by analyzing the
                     existing literature and discussion forums, we have assessed their relevance with a
                     survey involving 68 participants. Then, we have analyzed the occurrence of the studied
                     smells in 100 open-source Unity projects, and also assessed UnityLinter's accuracy.
                     Results of our empirical investigation indicate that developers well-received performance-
                     and behavior-related issues, while some maintainability issues are more controversial.
                     UnityLinter is, in general, accurate enough in detecting smells (86%-100% precision
                     and 50%-100% recall), and our study shows that the studied smell types occur in 39%-97%
                     of the analyzed projects.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387478">Detecting and Characterizing Bots that Commit Code</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Tapajit Dey</li>
               <li class="nameList">Sara Mousavi</li>
               <li class="nameList">Eduardo Ponce</li>
               <li class="nameList">Tanner Fry</li>
               <li class="nameList">Bogdan Vasilescu</li>
               <li class="nameList">Anna Filippova</li>
               <li class="nameList Last">Audris Mockus</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Background: Some developer activity traditionally performed manually, such as making
                     code commits, opening, managing, or closing issues is increasingly subject to automation
                     in many OSS projects. Specifically, such activity is often performed by tools that
                     react to events or run at specific times. We refer to such automation tools as bots
                     and, in many software mining scenarios related to developer productivity or code quality,
                     it is desirable to identify bots in order to separate their actions from actions of
                     individuals. Aim: Find an automated way of identifying bots and code committed by
                     these bots, and to characterize the types of bots based on their activity patterns.
                     Method and Result: We propose BIMAN, a systematic approach to detect bots using author
                     names, commit messages, files modified by the commit, and projects associated with
                     the commits. For our test data, the value for AUC-ROC was 0.9. We also characterized
                     these bots based on the time patterns of their code commits and the types of files
                     modified, and found that they primarily work with documentation files and web pages,
                     and these files are most prevalent in HTML and JavaScript ecosystems. We have compiled
                     a shareable dataset containing detailed information about 461 bots we found (all of
                     which have more than 1000 commits) and 13,762,430 commits they created.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387457">Developer-Driven Code Smell Prioritization</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Fabiano Pecorelli</li>
               <li class="nameList">Fabio Palomba</li>
               <li class="nameList">Foutse Khomh</li>
               <li class="nameList Last">Andrea De Lucia</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Code smells are symptoms of poor implementation choices applied during software evolution.
                     While previous research has devoted effort in the definition of automated solutions
                     to detect them, still little is known on how to support developers when prioritizing
                     them. Some works attempted to deliver solutions that can rank smell instances based
                     on their severity, computed on the basis of software metrics. However, this may not
                     be enough since it has been shown that the recommendations provided by current approaches
                     do not take the developer's perception of design issues into account. In this paper,
                     we perform a first step toward the concept of developer-driven code smell prioritization
                     and propose an approach based on machine learning able to rank code smells according
                     to the perceived criticality that developers assign to them. We evaluate our technique
                     in an empirical study to investigate its accuracy and the features that are more relevant
                     for classifying the developer's perception. Finally, we compare our approach with
                     a state-of-the-art technique. Key findings show that the our solution has an F-Measure
                     up to 85% and outperforms the baseline approach.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387471">Did You Remember To Test Your Tokens?</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Danielle Gonzalez</li>
               <li class="nameList">Michael Rath</li>
               <li class="nameList Last">Mehdi Mirakhorli</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Authentication is a critical security feature for confirming the identity of a system's
                     users, typically implemented with help from frameworks like Spring Security. It is
                     a complex feature which should be robustly tested at all stages of development. Unit
                     testing is an effective technique for fine-grained verification of feature behaviors
                     that is not widely-used to test authentication. Part of the problem is that resources
                     to help developers unit test security features are limited. Most security testing
                     guides recommend test cases in a "black box" or penetration testing perspective. These
                     resources are not easily applicable to developers writing new unit tests, or who want
                     a security-focused perspective on coverage.
                  </p> 
                  <p>In this paper, we address these issues by applying a grounded theory-based approach
                     to identify common (unit) test cases for token authentication through analysis of
                     481 JUnit tests exercising Spring Security-based authentication implementations from
                     53 open source Java projects. The outcome of this study is a developer-friendly unit
                     testing guide organized as a catalog of 53 test cases for token authentication, representing
                     unique combinations of 17 scenarios, 40 conditions, and 30 expected outcomes learned
                     from the data set in our analysis. We supplement the test guide with common test smells
                     to avoid. To verify the accuracy and usefulness of our testing guide, we sought feedback
                     from selected developers, some of whom authored unit tests in our dataset.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387445">Embedding Java Classes with code2vec: Improvements from Variable Obfuscation</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Rhys Compton</li>
               <li class="nameList">Eibe Frank</li>
               <li class="nameList">Panos Patros</li>
               <li class="nameList Last">Abigail Koay</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Automatic source code analysis in key areas of software engineering, such as code
                     security, can benefit from Machine Learning (ML). However, many standard ML approaches
                     require a numeric representation of data and cannot be applied directly to source
                     code. Thus, to enable ML, we need to embed source code into numeric feature vectors
                     while maintaining the semantics of the code as much as possible. code2vec is a recently
                     released embedding approach that uses the proxy task of method name prediction to
                     map Java methods to feature vectors. However, experimentation with code2vec shows
                     that it learns to rely on variable names for prediction, causing it to be easily fooled
                     by typos or adversarial attacks. Moreover, it is only able to embed individual Java
                     methods and cannot embed an entire collection of methods such as those present in
                     a typical Java class, making it difficult to perform predictions at the class level
                     (e.g., for the identification of malicious Java classes). Both shortcomings are addressed
                     in the research presented in this paper. We investigate the effect of obfuscating
                     variable names during training of a code2vec model to force it to rely on the structure
                     of the code rather than specific names and consider a simple approach to creating
                     class-level embeddings by aggregating sets of method embeddings. Our results, obtained
                     on a challenging new collection of source-code classification problems, indicate that
                     obfuscating variable names produces an embedding model that is both impervious to
                     variable naming and more accurately reflects code semantics. The datasets, models,
                     and code are shared1 for further ML research on source code.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387460">Empirical Study of Restarted and Flaky Builds on Travis CI</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Thomas Durieux</li>
               <li class="nameList">Claire Le Goues</li>
               <li class="nameList">Michael Hilton</li>
               <li class="nameList Last">Rui Abreu</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Continuous Integration (CI) is a development practice where developers frequently
                     integrate code into a common codebase. After the code is integrated, the CI server
                     runs a test suite and other tools to produce a set of reports (e.g., the output of
                     linters and tests). If the result of a CI test run is unexpected, developers have
                     the option to manually restart the build, re-running the same test suite on the same
                     code; this can reveal build flakiness, if the restarted build outcome differs from
                     the original build.
                  </p> 
                  <p>In this study, we analyze restarted builds, flaky builds, and their impact on the
                     development workflow. We observe that developers restart at least 1.72% of builds,
                     amounting to 56,522 restarted builds in our Travis CI dataset. We observe that more
                     mature and more complex projects are more likely to include restarted builds. The
                     restarted builds are mostly builds that are initially failing due to a test, network
                     problem, or a Travis CI limitations such as execution timeout. Finally, we observe
                     that restarted builds have an impact on development workflow. Indeed, in 54.42% of
                     the restarted builds, the developers analyze and restart a build within an hour of
                     the initial build execution. This suggests that developers wait for CI results, interrupting
                     their workflow to address the issue. Restarted builds also slow down the merging of
                     pull requests by a factor of three, bringing median merging time from 16h to 48h.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387462">Ethical Mining: A Case Study on MSR Mining Challenges</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Nicolas E. Gold</li>
               <li class="nameList Last">Jens Krinke</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Research in Mining Software Repositories (MSR) is research involving human subjects,
                     as the repositories usually contain data about developers' interactions with the repositories.
                     Therefore, any research in the area needs to consider the ethics implications of the
                     intended activity before starting. This paper presents a discussion of the ethics
                     implications of MSR research, using the mining challenges from the years 2010 to 2019
                     as a case study to identify the kinds of data used. It highlights problems that one
                     may encounter in creating such datasets, and discusses ethics challenges that may
                     be encountered when using existing datasets, based on a contemporary research ethics
                     framework. We suggest that the MSR community should increase awareness of ethics issues
                     by openly discussing ethics considerations in published articles.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387450">Forking Without Clicking: on How to Identify Software Repository Forks</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Antoine Pietri</li>
               <li class="nameList">Guillaume Rousseau</li>
               <li class="nameList Last">Stefano Zacchiroli</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The notion of software "fork" has been shifting over time from the (negative) phenomenon
                     of community disagreements that result in the creation of separate development lines
                     and ultimately software products, to the (positive) practice of using distributed
                     version control system (VCS) repositories to collaboratively improve a single product
                     without stepping on each others toes. In both cases the VCS repositories participating
                     in a fork share parts of a common development history.
                  </p> 
                  <p>Studies of software forks generally rely on hosting platform metadata, such as GitHub,
                     as the source of truth for what constitutes a fork. These "forge forks" however can
                     only identify as forks repositories that have been created on the platform, e.g.,
                     by clicking a "fork" button on the platform user interface. The increased diversity
                     in code hosting platforms (e.g., GitLab) and the habits of significant development
                     communities (e.g., the Linux kernel, which is not primarily hosted on any single platform)
                     call into question the reliability of trusting code hosting platforms to identify
                     forks. Doing so might introduce selection and methodological biases in empirical studies.
                  </p> 
                  <p>In this article we explore various definitions of "software forks", trying to capture
                     forking workflows that exist in the real world. We quantify the differences in how
                     many repositories would be identified as forks on GitHub according to the various
                     definitions, confirming that a significant number could be overlooked by only considering
                     forge forks. We study the structure and size of fork networks, observing how they
                     are affected by the proposed definitions and discuss the potential impact on empirical
                     research.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387439">From Innovations to Prospects: What Is Hidden Behind Cryptocurrencies?</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Ang Jia</li>
               <li class="nameList">Ming Fan</li>
               <li class="nameList">Xi Xu</li>
               <li class="nameList">Di Cui</li>
               <li class="nameList">Wenying Wei</li>
               <li class="nameList">Zijiang Yang</li>
               <li class="nameList">Kai Ye</li>
               <li class="nameList Last">Ting Liu</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The great influence of Bitcoin has promoted the rapid development of blockchain-based
                     digital currencies, especially the altcoins, since 2013. However, most altcoins share
                     similar source codes, resulting in concerns about code innovations. In this paper,
                     an empirical study on existing altcoins is carried out to offer a thorough understanding
                     of various aspects associated with altcoin innovations. Firstly, we construct the
                     dataset of altcoins, including source code repository, GitHub fork relation, and market
                     capitalization (cap). Then, we analyze the altcoin innovations from the perspective
                     of source code similarities. The results demonstrate that more than 85% of altcoin
                     repositories present high code similarities. Next, a temporal clustering algorithm
                     is proposed to mine the inheritance relationship among various altcoins. The family
                     pedigrees of altcoin are constructed, in which the altcoin presents similar evolution
                     features as biology, such as power-law in family size, variety in family evolution,
                     etc. Finally, we investigate the correlation between code innovations and market capitalization.
                     Although we fail to predict the price of altcoins based on their code similarities,
                     the results show that altcoins with higher innovations reflect better market prospects.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387449">Improved Automatic Summarization of Subroutines via Attention to File Context</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Sakib Haque</li>
               <li class="nameList">Alexander LeClair</li>
               <li class="nameList">Lingfei Wu</li>
               <li class="nameList Last">Collin McMillan</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Software documentation largely consists of short, natural language summaries of the
                     subroutines in the software. These summaries help programmers quickly understand what
                     a subroutine does without having to read the source code him or herself. The task
                     of writing these descriptions is called "source code summarization" and has been a
                     target of research for several years. Recently, AI-based approaches have superseded
                     older, heuristic-based approaches. Yet, to date these AI-based approaches assume that
                     all the content needed to predict summaries is inside subroutine itself. This assumption
                     limits performance because many subroutines cannot be understood without surrounding
                     context. In this paper, we present an approach that models the file context of subroutines
                     (i.e. other subroutines in the same file) and uses an attention mechanism to find
                     words and concepts to use in summaries. We show in an experiment that our approach
                     extends and improves several recent baselines.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387453">Investigating Severity Thresholds for Test Smells</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Davide Spadini</li>
               <li class="nameList">Martin Schvarcbacher</li>
               <li class="nameList">Ana-Maria Oprescu</li>
               <li class="nameList">Magiel Bruntink</li>
               <li class="nameList Last">Alberto Bacchelli</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Test smells are poor design decisions implemented in test code, which can have an
                     impact on the effectiveness and maintainability of unit tests. Even though test smell
                     detection tools exist, how to rank the severity of the detected smells is an open
                     research topic. In this work, we aim at investigating the severity rating for four
                     test smells and investigate their perceived impact on test suite maintainability by
                     the developers. To accomplish this, we first analyzed some 1,500 open-source projects
                     to elicit severity thresholds for commonly found test smells. Then, we conducted a
                     study with developers to evaluate our thresholds. We found that (1) current detection
                     rules for certain test smells are considered as too strict by the developers and (2)
                     our newly defined severity thresholds are in line with the participants' perception
                     of how test smells have an impact on the maintainability of a test suite. Preprint
                     [https://doi.org/10.5281/zenodo.3744281], data and material [https://doi.org/10.5281/zenodo.3611111].
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387466">Need for Tweet: How Open Source Developers Talk About Their GitHub Work on Twitter</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Hongbo Fang</li>
               <li class="nameList">Daniel Klug</li>
               <li class="nameList">Hemank Lamba</li>
               <li class="nameList">James Herbsleb</li>
               <li class="nameList Last">Bogdan Vasilescu</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Social media, especially Twitter, has always been a part of the professional lives
                     of software developers, with prior work reporting on a diversity of usage scenarios,
                     including sharing information, staying current, and promoting one's work. However,
                     previous studies of Twitter use by software developers typically lack information
                     about activities of the study subjects (and their outcomes) on other platforms. To
                     enable such future research, in this paper we propose a computational approach to
                     cross-link users across Twitter and GitHub, revealing (at least) 70,427 users active
                     on both. As a preliminary analysis of this dataset, we report on a case study of 786
                     tweets by open-source developers about GitHub work, combining automatic characterization
                     of tweet authors in terms of their relationship to the GitHub items linked in their
                     tweets with qualitative analysis of the tweet contents. We find that different developer
                     roles tend to have different tweeting behaviors, with repository owners being perhaps
                     the most distinctive group compared to other project contributors and followers. We
                     also note a sizeable group of people who follow others on GitHub and tweet about these
                     people's work, but do not otherwise contribute to those open-source projects. Our
                     results and public dataset open up multiple future research directions.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387467">On the Prevalence, Impact, and Evolution of SQL Code Smells in Data-Intensive Systems</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Biruk Asmare Muse</li>
               <li class="nameList">Mohammad Masudur Rahman</li>
               <li class="nameList">Csaba Nagy</li>
               <li class="nameList">Anthony Cleve</li>
               <li class="nameList">Foutse Khomh</li>
               <li class="nameList Last">Giuliano Antoniol</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Code smells indicate software design problems that harm software quality. Data-intensive
                     systems that frequently access databases often suffer from SQL code smells besides
                     the traditional smells. While there have been extensive studies on traditional code
                     smells, recently, there has been a growing interest in SQL code smells. In this paper,
                     we conduct an empirical study to investigate the prevalence and evolution of SQL code
                     smells in open-source, data-intensive systems. We collected 150 projects and examined
                     both traditional and SQL code smells in these projects. Our investigation delivers
                     several important findings. First, SQL code smells are indeed prevalent in data-intensive
                     software systems. Second, SQL code smells have a weak co-occurrence with traditional
                     code smells. Third, SQL code smells have a weaker association with bugs than that
                     of traditional code smells. Fourth, SQL code smells are more likely to be introduced
                     at the beginning of the project lifetime and likely to be left in the code without
                     a fix, compared to traditional code smells. Overall, our results show that SQL code
                     smells are indeed prevalent and persistent in the studied data-intensive software
                     systems. Developers should be aware of these smells and consider detecting and refactoring
                     SQL code smells and traditional code smells separately, using dedicated tools.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387456">On the Relationship between User Churn and Software Issues</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Omar El Zarif</li>
               <li class="nameList">Daniel Alencar Da Costa</li>
               <li class="nameList">Safwat Hassan</li>
               <li class="nameList Last">Ying Zou</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The satisfaction of users is only part of the success of a software product, since
                     a strong competition can easily detract users from a software product/service. User
                     churn is the jargon used to denote when a user changes from a product/service to the
                     one offered by the competition. In this study, we empirically investigate the relationship
                     between the issues that are present in a software product and user churn. For this
                     purpose, we investigate a new dataset provided by the alternativeto.net platform.
                     Alternativeto.net has a unique feature that allows users to recommend alternatives
                     for a specific software product, which signals the intention to switch from one software
                     product to another. Through our empirical study, we observe that (i) the intention
                     to change software is tightly associated to the issues that are present in these software;
                     (ii) we can predict the rate of potential churn using machine learning models; (iii)
                     the longer the issue takes to be fixed, the higher the chances of user churn; and
                     (iv) issues within more general software modules are more likely to be associated
                     with user churn. Our study can provide more insights on the prioritization of issues
                     that need to be fixed to proactively minimize the chances of user churn.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387443">PUMiner: Mining Security Posts from Developer Question and Answer Websites with PU Learning</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Triet Huynh Minh Le</li>
               <li class="nameList">David Hin</li>
               <li class="nameList">Roland Croft</li>
               <li class="nameList Last">M. Ali Babar</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Security is an increasing concern in software development. Developer Question and
                     Answer (Q&amp;A) websites provide a large amount of security discussion. Existing studies
                     have used human-defined rules to mine security discussions, but these works still
                     miss many posts, which may lead to an incomplete analysis of the security practices
                     reported on Q&amp;A websites. Traditional supervised Machine Learning methods can automate
                     the mining process; however, the required negative (non-security) class is too expensive
                     to obtain. We propose a novel learning framework, PUMiner, to automatically mine security
                     posts from Q&amp;A websites. PUMiner builds a context-aware embedding model to extract
                     features of the posts, and then develops a two-stage PU model to identify security
                     content using the labelled Positive and Un-labelled posts. We evaluate PUMiner on
                     more than 17.2 million posts on Stack Overflow and 52,611 posts on Security StackExchange.
                     We show that PUMiner is effective with the validation performance of at least 0.85
                     across all model configurations. Moreover, Matthews Correlation Coefficient (MCC)
                     of PUMiner is 0.906, 0.534 and 0.084 points higher than one-class SVM, positive-similarity
                     filtering, and one-stage PU models on unseen testing posts, respectively. PUMiner
                     also performs well with an MCC of 0.745 for scenarios where string matching totally
                     fails. Even when the ratio of the labelled positive posts to the un-labelled ones
                     is only 1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than
                     fully-supervised learning. Using PUMiner, we provide the largest and up-to-date security
                     content on Q&amp;A websites for practitioners and researchers.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387452">Painting Flowers: Reasons for Using Single-State State Machines in Model-Driven Engineering</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Nan Yang</li>
               <li class="nameList">Pieter Cuijpers</li>
               <li class="nameList">Ramon Schiffelers</li>
               <li class="nameList">Johan Lukkien</li>
               <li class="nameList Last">Alexander Serebrenik</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Models, as the main artifact in model-driven engineering, have been extensively used
                     in the area of embedded systems for code generation and verification. One of the most
                     popular behavioral modeling techniques is state machine. Many state machine modeling
                     guidelines recommend that a state machine should have more than one state in order
                     to be meaningful. However, single-state state machines (SSSMs) violating this recommendation
                     have been used in modeling cases reported in the literature.
                  </p> 
                  <p>We study the prevalence and role of SSSMs in the domain of embedded systems, as well
                     as the reasons why developers use them and their perceived advantages and disadvantages.
                     We employ the sequential explanatory strategy to study 1500 state machines from 26
                     components at ASML, a leading company in manufacturing lithography machines from the
                     semiconductor industry. We observe that 25 out of 26 components contain SSSMs, making
                     up 25.3% of the model base. To understand the reasons for this extensive usage we
                     conduct a series of interviews followed by a grounded theory building. The results
                     suggest that SSSMs are used to interface with the existing code, to deal with tool
                     limitations, to facilitate maintenance and to ease verification. Based on our results,
                     we provide implications to modeling tool builders. Furthermore, we formulate two hypotheses
                     about the effectiveness of SSSMs as well as the impacts of SSSMs on development, maintenance
                     and verification.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387481">Polyglot and Distributed Software Repository Mining with Crossflow</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Konstantinos Barmpis</li>
               <li class="nameList">Patrick Neubauer</li>
               <li class="nameList">Jonathan Co</li>
               <li class="nameList">Dimitris Kolovos</li>
               <li class="nameList">Nicholas Matragkas</li>
               <li class="nameList Last">Richard F. Paige</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Mining software repositories at a large scale typically requires substantial computational
                     and storage resources. This creates an increasing need for repository mining programs
                     to be executed in a distributed manner, such that remote collaborators can contribute
                     local computational and storage resources. In this paper we present Crossflow, a novel
                     framework for building polyglot distributed repository mining programs. We demonstrate
                     how Crossflow offers delegation of mining jobs to remote workers and can cache their
                     results, how such workers are able to implement advanced behavior like load balancing
                     and rejecting jobs they either cannot perform or would execute sub-optimally, and
                     how workers of the same analysis program can be written in different programing languages
                     like Java and Python, executing only relevant parts of the program described in that
                     language.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387458">RTPTorrent: An Open-source Dataset for Evaluating Regression Test Prioritization</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Toni Mattis</li>
               <li class="nameList">Patrick Rein</li>
               <li class="nameList">Falco Dürsch</li>
               <li class="nameList Last">Robert Hirschfeld</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The software engineering practice of automated testing helps programmers find defects
                     earlier during development. With growing software projects and longer-running test
                     suites, frequency and immediacy of feedback decline, thereby making defects harder
                     to repair. Regression test prioritization (RTP) is concerned with running relevant
                     tests earlier to lower the costs of defect localization and to improve feedback.
                  </p> 
                  <p>Finding representative data to evaluate RTP techniques is non-trivial, as most software
                     is published without failing tests. In this work, we systematically survey a wide
                     range of RTP literature regarding whether their dataset uses real or synthetic defects
                     or tests, whether they are publicly available, and whether datasets are reused. We
                     observed that some datasets are reused, however, many projects study only few projects
                     and these rarely resemble real-world development activity.
                  </p> 
                  <p>In light of these threats to ecological validity, we describe the construction and
                     characteristics of a new dataset, named RTPTorrent, based on 20 open-source Java programs.
                  </p> 
                  <p>Our dataset allows researchers to evaluate prioritization heuristics based on version
                     control meta-data, source code, and test results from fine-grained, automated builds
                     over 9 years of development history. We provide reproducible baselines for initial
                     comparisons and make all data publicly available.
                  </p> 
                  <p>We see this as a step towards better reproducibility, ecological validity, and long-term
                     availability of studied software in the field of test prioritization.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387444">SoftMon: A Tool to Compare Similar Open-source Software from a Performance Perspective</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Shubhankar Suman Singh</li>
               <li class="nameList Last">Smruti R. Sarangi</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Over the past two decades, a rich ecosystem of open-source software has evolved. For
                     every type of application, there are a wide variety of alternatives. We observed that
                     even if different applications that perform similar tasks and compiled with the same
                     versions of the compiler and the libraries, they perform very differently while running
                     on the same system. Sadly prior work in this area that compares two code bases for
                     similarities does not help us in finding the reasons for the differences in performance.
                  </p> 
                  <p>In this paper, we develop a tool, SoftMon, that can compare the codebases of two separate
                     applications and pinpoint the exact set of functions that are disproportionately responsible
                     for differences in performance. Our tool uses machine learning and NLP techniques
                     to analyze why a given open-source application has a lower performance as compared
                     to its peers, design bespoke applications that can incorporate specific innovations
                     (identified by SoftMon) in competing applications, and diagnose performance bugs.
                  </p> 
                  <p>In this paper, we compare a wide variety of large open-source programs such as image
                     editors, audio players, text editors, PDF readers, mail clients and even full-fledged
                     operating systems (OSs). In all cases, our tool was able to pinpoint a set of at the
                     most 10-15 functions that are responsible for the differences within 200 seconds.
                     A subsequent manual analysis assisted by our graph visualization engine helps us find
                     the reasons. We were able to validate most of the reasons by correlating them with
                     subsequent observations made by developers or from existing technical literature.
                     The manual phase of our analysis is limited to 30 minutes (tested with human subjects).
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387465">The Impact of a Major Security Event on an Open Source Project: The Case of OpenSSL</a></h3>
            <ul class="DLauthors">
               <li class="nameList Last">James Walden</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Context: The Heartbleed vulnerability brought OpenSSL to international attention in
                     2014. The almost moribund project was a key security component in public web servers
                     and over a billion mobile devices. This vulnerability led to new investments in OpenSSL.
                  </p> 
                  <p>Objective: The goal of this study is to determine how the Heart-bleed vulnerability
                     changed the software evolution of OpenSSL. We study changes in vulnerabilities, code
                     quality, project activity, and software engineering practices.
                  </p> 
                  <p>Method: We use a mixed methods approach, collecting multiple types of quantitative
                     data and qualitative data from web sites and an interview with a developer who worked
                     on post-Heartbleed changes. We use regression discontinuity analysis to determine
                     changes in levels and slopes of code and project activity metrics resulting from Heartbleed.
                  </p> 
                  <p>Results: The OpenSSL project made tremendous improvements to code quality and security
                     after Heartbleed. By the end of 2016, the number of commits per month had tripled,
                     91 vulnerabilities were found and fixed, code complexity decreased significantly,
                     and OpenSSL obtained a CII best practices badge, certifying its use of good open source
                     development practices.
                  </p> 
                  <p>Conclusions: The OpenSSL project provides a model of how an open source project can
                     adapt and improve after a security event. The evolution of OpenSSL shows that the
                     number of known vulnerabilities is not a useful indicator of project security. A small
                     number of vulnerabilities may simply indicate that a project does not expend much
                     effort to finding vulnerabilities. This study suggests that project activity and CII
                     badge best practices may be better indicators of code quality and security than vulnerability
                     counts.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387479">The Scent of Deep Learning Code: An Empirical Study</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Hadhemi Jebnoun</li>
               <li class="nameList">Houssem Ben Braiek</li>
               <li class="nameList">Mohammad Masudur Rahman</li>
               <li class="nameList Last">Foutse Khomh</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Deep learning practitioners are often interested in improving their model accuracy
                     rather than the interpretability of their models. As a result, deep learning applications
                     are inherently complex in their structures. They also need to continuously evolve
                     in terms of code changes and model updates. Given these confounding factors, there
                     is a great chance of violating the recommended programming practices by the developers
                     in their deep learning applications. In particular, the code quality might be negatively
                     affected due to their drive for the higher model performance. Unfortunately, the code
                     quality of deep learning applications has rarely been studied to date. In this paper,
                     we conduct an empirical study to investigate the distribution of code smells in deep
                     learning applications. To this end, we perform a comparative analysis between deep
                     learning and traditional open-source applications collected from GitHub. We have several
                     major findings. First, long lambda expression, long ternary conditional expression,
                     and complex container comprehension smells are frequently found in deep learning projects.
                     That is, deep learning code involves more complex or longer expressions than the traditional
                     code does. Second, the number of code smells increases across the releases of deep
                     learning applications. Third, we found that there is a co-existence between code smells
                     and software bugs in the studied deep learning code, which confirms our conjecture
                     on the degraded code quality of deep learning applications.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387473">The State of the ML-universe: 10 Years of Artificial Intelligence &amp; Machine Learning Software Development on GitHub</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Danielle Gonzalez</li>
               <li class="nameList">Thomas Zimmermann</li>
               <li class="nameList Last">Nachiappan Nagappan</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>In the last few years, artificial intelligence (AI) and machine learning (ML) have
                     become ubiquitous terms. These powerful techniques have escaped obscurity in academic
                     communities with the recent onslaught of AI &amp; ML tools, frameworks, and libraries
                     that make these techniques accessible to a wider audience of developers. As a result,
                     applying AI &amp; ML to solve existing and emergent problems is an increasingly popular
                     practice. However, little is known about this domain from the software engineering
                     perspective. Many AI &amp; ML tools and applications are open source, hosted on platforms
                     such as GitHub that provide rich tools for large-scale distributed software development.
                     Despite widespread use and popularity, these repositories have never been examined
                     as a community to identify unique properties, development patterns, and trends.
                  </p> 
                  <p>In this paper, we conducted a large-scale empirical study of AI &amp; ML Tool (700) and
                     Application (4,524) repositories hosted on GitHub to develop such a characterization.
                     While not the only platform hosting AI &amp; ML development, GitHub facilitates collecting
                     a rich data set for each repository with high traceability between issues, commits,
                     pull requests and users. To compare the AI &amp; ML community to the wider population
                     of repositories, we also analyzed a set of 4,101 unrelated repositories. We enhance
                     this characterization with an elaborate study of developer workflow that measures
                     collaboration and autonomy within a repository. We've captured key insights of this
                     community's 10 year history such as it's primary language (Python) and most popular
                     repositories (Tensorflow, Tesseract). Our findings show the AI &amp; ML community has
                     unique characteristics that should be accounted for in future research.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387440">Traceability Support for Multi-Lingual Software Projects</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Yalin Liu</li>
               <li class="nameList">Jinfeng Lin</li>
               <li class="nameList Last">Jane Cleland-Huang</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Software traceability establishes associations between diverse software artifacts
                     such as requirements, design, code, and test cases. Due to the non-trivial costs of
                     manually creating and maintaining links, many researchers have proposed automated
                     approaches based on information retrieval techniques. However, many globally distributed
                     software projects produce software artifacts written in two or more languages. The
                     use of intermingled languages reduces the efficacy of automated tracing solutions.
                     In this paper, we first analyze and discuss patterns of intermingled language use
                     across multiple projects, and then evaluate several different tracing algorithms including
                     the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation
                     (LDA), and various models that combine mono-and cross-lingual word embeddings with
                     the Generative Vector Space Model (GVSM). Based on an analysis of 14 Chinese-English
                     projects, our results show that best performance is achieved using mono-lingual word
                     embeddings integrated into GVSM with machine translation as a preprocessing step.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387447">Using Large-Scale Anomaly Detection on Code to Improve Kotlin Compiler</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Timofey Bryksin</li>
               <li class="nameList">Victor Petukhov</li>
               <li class="nameList">Ilya Alexin</li>
               <li class="nameList">Stanislav Prikhodko</li>
               <li class="nameList">Alexey Shpilman</li>
               <li class="nameList">Vladimir Kovalenko</li>
               <li class="nameList Last">Nikita Povarov</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>In this work, we apply anomaly detection to source code and byte-code to facilitate
                     the development of a programming language and its compiler. We define anomaly as a
                     code fragment that is different from typical code written in a particular programming
                     language. Identifying such code fragments is beneficial to both language developers
                     and end users, since anomalies may indicate potential issues with the compiler or
                     with runtime performance. Moreover, anomalies could correspond to problems in language
                     design. For this study, we choose Kotlin as the target programming language. We outline
                     and discuss approaches to obtaining vector representations of source code and bytecode
                     and to the detection of anomalies across vectorized code snippets. The paper presents
                     a method that aims to detect two types of anomalies: syntax tree anomalies and so-called
                     compiler-induced anomalies that arise only in the compiled bytecode. We describe several
                     experiments that employ different combinations of vectorization and anomaly detection
                     techniques and discuss types of detected anomalies and their usefulness for language
                     developers. We demonstrate that the extracted anomalies and the underlying extraction
                     technique provide additional value for language development.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387476">Using Others' Tests to Identify Breaking Updates</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Suhaib Mujahid</li>
               <li class="nameList">Rabe Abdalkareem</li>
               <li class="nameList">Emad Shihab</li>
               <li class="nameList Last">Shane McIntosh</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The reuse of third-party packages has become a common practice in contemporary software
                     development. Software dependencies are constantly evolving with newly added features
                     and patches that fix bugs in older versions. However, updating dependencies could
                     introduce new bugs or break backward compatibility. In this work, we propose a technique
                     to detect breakage-inducing versions of third-party dependencies. The key insight
                     behind our approach is to leverage the automated test suites of other projects that
                     depend upon the same dependency to test newly released versions. We conjecture that
                     this crowd-based approach will help to detect breakage-inducing versions because it
                     broadens the set of realistic usage scenarios to which a package version has been
                     exposed. To evaluate our conjecture, we perform an empirical study of 391,553 npm
                     packages. We use the dependency network from these packages to identify candidate
                     tests of third-party packages. Moreover, to evaluate our proposed technique, we mine
                     the history of this dependency network to identify ten breakage-inducing versions.
                     We find that our proposed technique can detect six of the ten studied breakage-inducing
                     versions. Our findings can help developers to make more informed decisions when they
                     update their dependencies.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387451">Visualization of Methods Changeability Based on VCS Data</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Sergey Svitkov</li>
               <li class="nameList Last">Timofey Bryksin</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Software engineers have a wide variety of tools and techniques that can help them
                     improve the quality of their code, but still, a lot of bugs remain undetected. In
                     this paper we build on the idea that if a particular fragment of code is changed too
                     often, it could be caused by some technical or architectural issues, therefore, this
                     fragment requires additional attention from developers. Most teams nowadays use version
                     control systems to track changes in their code and organize cooperation between developers.
                     We propose to use data from version control systems to track the number of changes
                     for each method in a project for a selected time period and display this information
                     within the IDE's code editor. The paper describes such a tool called Topias built
                     as a plugin for IntelliJ IDEA. Its source code is available at https://github.com/JetBrains-Research/topias.
                     A demonstration video can be found at https://www.youtube.com/watch?v=xsqc4gCTxfA.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387442">What constitutes Software?: An Empirical, Descriptive Study of Artifacts</a></h3>
            <ul class="DLauthors">
               <li class="nameList Last">Rolf-Helge Pfeiffer</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The term software is ubiquitous, however, it does not seem as if we as a community
                     have a clear understanding of what software actually is. Imprecise definitions of
                     software do not help other professions, in particular those acquiring and sourcing
                     software from third-parties, when deciding what precisely are potential deliverables.
                     In this paper we investigate which artifacts constitute software by analyzing 23 715
                     repositories from Github, we categorize the found artifacts into high-level categories,
                     such as, code, data, and documentation (and into 19 more concrete categories) and
                     we can confirm the notion of others that software is more than just source code or
                     programs, for which the term is often used synonymously. With this work we provide
                     an empirical study of more than 13 million artifacts, we provide a taxonomy of artifact
                     categories, and we can conclude that software most often consists of variously distributed
                     amounts of code in different forms, such as source code, binary code, scripts, etc.,
                     data, such as configuration files, images, databases, etc., and documentation, such
                     as user documentation, licenses, etc.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387482">What is the Vocabulary of Flaky Tests?</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Gustavo Pinto</li>
               <li class="nameList">Breno Miranda</li>
               <li class="nameList">Supun Dissanayake</li>
               <li class="nameList">Marcelo d'Amorim</li>
               <li class="nameList">Christoph Treude</li>
               <li class="nameList Last">Antonia Bertolino</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research
                     activity on this topic, no effort has been made on understanding the vocabulary of
                     flaky tests. This work proposes to automatically classify tests as flaky or not based
                     on their vocabulary. Static classification of flaky tests is important, for example,
                     to detect the introduction of flaky tests and to search for flaky tests after they
                     are introduced in regression test suites.
                  </p> 
                  <p>We evaluated performance of various machine learning algorithms to solve this problem.
                     We constructed a data set of flaky and non-flaky tests by running every test case,
                     in a set of 64k tests, 100 times (6.4 million test executions). We then used machine
                     learning techniques on the resulting data set to predict which tests are flaky from
                     their source code. Based on features, such as counting stemmed tokens extracted from
                     source code identifiers, we achieved an F-measure of 0.95 for the identification of
                     flaky tests. The best prediction performance was obtained when using Random Forest
                     and Support Vector Machines. In terms of the code identifiers that are most strongly
                     associated with test flakiness, we noted that job, action, and services are commonly
                     associated with flaky tests. Overall, our results provides initial yet strong evidence
                     that static detection of flaky tests is effective.
                  </p>
                  	
               </div>
            </div>
            						
            					
            <h2>SESSION: Data Showcase</h2>
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387487">20-MAD: 20 Years of Issues and Commits of Mozilla and Apache Development</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Maëlick Claes</li>
               <li class="nameList Last">Mika V. Mäntylä</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Data of long-lived and high profile projects is valuable for research on successful
                     software engineering in the wild. Having a dataset with different linked software
                     repositories of such projects, enables deeper diving investigations. This paper presents
                     20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects.
                     It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues,
                     and 17.3M issue comments, and its compressed size is over 6 GB. The data contains
                     all the typical information about source code commits (e.g., lines added and removed,
                     message and commit time) and issues (status, severity, votes, and summary). The issue
                     comments have been pre-processed for natural language processing and sentiment analysis.
                     This includes emoticons and valence and arousal scores. Linking code repository and
                     issue tracker information, allows studying individuals in two types of repositories
                     and provide more accurate time zone information for issue trackers as well. To our
                     knowledge, this the largest linked dataset in size and in project lifetime that is
                     not based on GitHub.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387501">A C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Jiahao Fan</li>
               <li class="nameList">Yi Li</li>
               <li class="nameList">Shaohua Wang</li>
               <li class="nameList Last">Tien N. Nguyen</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>We collected a large C/C++ code vulnerability dataset from open-source Github projects,
                     namely Big-Vul. We crawled the public Common Vulnerabilities and Exposures (CVE) database
                     and CVE-related source code repositories. Specifically, we collected the descriptive
                     information of the vulnerabilities from the CVE database, e.g., CVE IDs, CVE severity
                     scores, and CVE summaries. With the CVE information and its related published Github
                     code repository links, we downloaded all of the code repositories and extracted vulnerability
                     related code changes. In total, Big-Vul contains 3,754 code vulnerabilities spanning
                     91 different vulnerability types. All these code vulnerabilities are extracted from
                     348 Github projects. All information is stored in the CSV format. We linked the code
                     changes with the CVE descriptive information. Thus, our Big-Vul can be used for various
                     research topics, e.g., detecting and fixing vulnerabilities, analyzing the vulnerability
                     related code changes. Big-Vul is publicly available on Github.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387499">A Complete Set of Related Git Repositories Identified via Community Detection Approaches
                  Based on Shared Commits</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Audris Mockus</li>
               <li class="nameList">Diomidis Spinellis</li>
               <li class="nameList">Zoe Kotti</li>
               <li class="nameList Last">Gabriel John Dusing</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>In order to understand the state and evolution of the entirety of open source software
                     we need to get a handle on the set of distinct software projects. Most of open source
                     projects presently utilize Git, which is a distributed version control system allowing
                     easy creation of clones and resulting in numerous repositories that are almost entirely
                     based on some parent repository from which they were cloned. Git commits are unlikely
                     to get produce and represent a way to group cloned repositories. We use World of Code
                     infrastructure containing approximately 2B commits and 100M repositories to create
                     and share such a map. We discover that the largest group contains almost 14M repositories
                     most of which are unrelated to each other. As it turns out, the developers can push
                     git object to an arbitrary repository or pull objects from unrelated repositories,
                     thus linking unrelated repositories. To address this, we apply Louvain community detection
                     algorithm to this very large graph consisting of links between commits and projects.
                     The approach successfully reduces the size of the megacluster with the largest group
                     of highly interconnected projects containing under 400K repositories. We expect that
                     the resulting map of related projects as well as tools and methods to handle the very
                     large graph will serve as a reference set for mining software projects and other applications.
                     Further work is needed to determine different types of relationships among projects
                     induced by shared commits and other relationships, for example, by shared source code
                     or similar filenames.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387500">A Dataset and an Approach for Identity Resolution of 38 Million Author IDs extracted
                  from 2B Git Commits</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Tanner Fry</li>
               <li class="nameList">Tapajit Dey</li>
               <li class="nameList">Andrey Karnauch</li>
               <li class="nameList Last">Audris Mockus</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The data collected from open source projects provide means to model large software
                     ecosystems, but often suffer from data quality issues, specifically, multiple author
                     identification strings in code commits might actually be associated with one developer.
                     While many methods have been proposed for addressing this problem, they are either
                     heuristics requiring manual tweaking, or require too much calculation time to do pairwise
                     comparisons for 38M author IDs in, for example, the World of Code collection. In this
                     paper, we propose a method that finds all author IDs belonging to a single developer
                     in this entire dataset, and share the list of all author IDs that were found to have
                     aliases. To do this, we first create blocks of potentially connected author IDs and
                     then use a machine learning model to predict which of these potentially related IDs
                     belong to the same developer. We processed around 38 million author IDs and found
                     around 14.8 million IDs to have an alias, which belong to 5.4 million different developers,
                     with the median number of aliases being 2 per developer. This dataset can be used
                     to create more accurate models of developer behaviour at the entire OSS ecosystem
                     level and can be used to provide a service to rapidly resolve new author IDs.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387496">A Dataset for GitHub Repository Deduplication</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Diomidis Spinellis</li>
               <li class="nameList">Zoe Kotti</li>
               <li class="nameList Last">Audris Mockus</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>GitHub projects can be easily replicated through the site's fork process or through
                     a Git clone-push sequence. This is a problem for empirical software engineering, because
                     it can lead to skewed results or mistrained machine learning models. We provide a
                     dataset of 10.6 million GitHub projects that are copies of others, and link each record
                     with the project's ultimate parent. The ultimate parents were derived from a ranking
                     along six metrics. The related projects were calculated as the connected components
                     of an 18.2 million node and 12 million edge denoised graph created by directing edges
                     to ultimate parents. The graph was created by filtering out more than 30 hand-picked
                     and 2.3 million pattern-matched clumping projects. Projects that introduced unwanted
                     clumping were identified by repeatedly visualizing shortest path distances between
                     unrelated important projects. Our dataset identified 30 thousand duplicate projects
                     in an existing popular reference dataset of 1.8 million projects. An evaluation of
                     our dataset against another created independently with different methods found a significant
                     overlap, but also differences attributed to the operational definition of what projects
                     are considered as related.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387498">A Dataset of Dockerfiles</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Jordan Henkel</li>
               <li class="nameList">Christian Bird</li>
               <li class="nameList">Shuvendu K. Lahiri</li>
               <li class="nameList Last">Thomas Reps</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Dockerfiles are one of the most prevalent kinds of DevOps artifacts used in industry.
                     Despite their prevalence, there is a lack of sophisticated semantics-aware static
                     analysis of Dockerfiles. In this paper, we introduce a dataset of approximately 178,000
                     unique Dockerfiles collected from GitHub. To enhance the usability of this data, we
                     describe five representations we have devised for working with, mining from, and analyzing
                     these Dockerfiles. Each Dockerfile representation builds upon the previous ones, and
                     the final representation, created by three levels of nested parsing and abstraction,
                     makes tasks such as mining and static checking tractable. The Dockerfiles, in each
                     of the five representations, along with metadata and the tools used to shepard the
                     data from one representation to the next are all available at: https://doi.org/10.5281/zenodo.3628771.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387495">A Dataset of Enterprise-Driven Open Source Software</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Diomidis Spinellis</li>
               <li class="nameList">Zoe Kotti</li>
               <li class="nameList">Konstantinos Kravvaritis</li>
               <li class="nameList">Georgios Theodorou</li>
               <li class="nameList Last">Panos Louridas</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>We present a dataset of open source software developed mainly by enterprises rather
                     than volunteers. This can be used to address known generalizability concerns, and,
                     also, to perform research on open source business software development. Based on the
                     premise that an enterprise's employees are likely to contribute to a project developed
                     by their organization using the email account provided by it, we mine domain names
                     associated with enterprises from open data sources as well as through white- and blacklisting,
                     and use them through three heuristics to identify 17 264 enterprise GitHub projects.
                     We provide these as a dataset detailing their provenance and properties. A manual
                     evaluation of a dataset sample shows an identification accuracy of 89%. Through an
                     exploratory data analysis we found that projects are staffed by a plurality of enterprise
                     insiders, who appear to be pulling more than their weight, and that in a small percentage
                     of relatively large projects development happens exclusively through enterprise insiders.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387492">A Mixed Graph-Relational Dataset of Socio-technical Interactions in Open Source Systems</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Usman Ashraf</li>
               <li class="nameList">Christoph Mayr-Dorn</li>
               <li class="nameList">Alexander Egyed</li>
               <li class="nameList Last">Sebastiano Panichella</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Several researchers have studied that developers contributing to open source systems
                     tend to self-organize in "emerging" teams. The structure of these latent teams has
                     a significant impact on software quality, with development teams structure somewhat
                     reflected in the way developers communicate and contribute in the subsystems of a
                     system. Therefore, in order to study socio-technical interactions as well as the software
                     evolution dynamics of open source systems, in this paper, we present a novel dataset,
                     gathered from 20 open source projects, which report the developers' activities in
                     the scope of commits and issues at the level of subsystems. Thus, the new, generated
                     dataset comprises of emerging and explicit links among developers, commits, issues,
                     and source code artifacts, with data grouped around the subsystems point of view,
                     which can be used to better study the system dynamics behind the extracted sociotechnical
                     interactions.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387489">On the Shoulders of Giants: A New Dataset for Pull-based Development Research</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Xunhui Zhang</li>
               <li class="nameList">Ayushi Rastogi</li>
               <li class="nameList Last">Yue Yu</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Pull-based development is a widely adopted paradigm for collaboration in distributed
                     software development, attracting eyeballs from both academic and industry. To better
                     study pull-based development model, this paper presents a new dataset containing 96
                     features collected from 11,230 projects and 3,347,937 pull requests. We describe the
                     creation process and explain the features in details. To the best of our knowledge,
                     our dataset is the most comprehensive and largest one toward a complete picture for
                     pull-based development research.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387503">AndroZooOpen: Collecting Large-scale Open Source Android Apps for the Research Community</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Pei Liu</li>
               <li class="nameList">Li Li</li>
               <li class="nameList">Yanjie Zhao</li>
               <li class="nameList">Xiaoyu Sun</li>
               <li class="nameList Last">John Grundy</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>It is critical for research to have an open, well-curated, representative set of apps
                     for analysis. We present a collection of open-source Android apps collected from several
                     sources, including Github. Our dataset, AndroZooOpen, currently contains over 45,000
                     app artefacts, a representative picture of Github-hosted Android apps. For apps released
                     on Google Play, metadata including categories, ratings and user reviews, are also
                     stored. We share this new dataset as part of our ongoing research to better support
                     and enable new research topics involving Android app artefact analysis, and as a supplement
                     dataset for AndroZoo, a well-known app collection of close-sourced Android apps.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387486">Dataset of Video Game Development Problems</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Cristiano Politowski</li>
               <li class="nameList">Fabio Petrillo</li>
               <li class="nameList">Gabriel Cavalheiro Ullmann</li>
               <li class="nameList">Josias de Andrade Werly</li>
               <li class="nameList Last">Yann-Gaël Guéhéneuc</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Different from traditional software development, there is little information about
                     the software-engineering process and techniques in video-game development. One popular
                     way to share knowledge among the video-game developers' community is the publishing
                     of postmortems, which are documents summarizing what happened during the video-game
                     development project. However, these documents are written without formal structure
                     and often providing disparate information. Through this paper, we provide developers
                     and researchers with grounded dataset describing software-engineering problems in
                     video-game development extracted from postmortems. We created the dataset using an
                     iterative method through which we manually coded more than 200 postmortems spanning
                     20 years (1998 to 2018) and extracted 1,035 problems related to software engineering
                     while maintaining traceability links to the postmortems. We grouped the problems in
                     20 different types. This dataset is useful to understand the problems faced by developers
                     during video-game development, providing researchers and practitioners a starting
                     point to study video-game development in the context of software engineering.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387490">Employing Contribution and Quality Metrics for Quantifying the Software Development
                  Process</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Themistoklis Diamantopoulos</li>
               <li class="nameList">Michail D. Papamichail</li>
               <li class="nameList">Thomas Karanikiotis</li>
               <li class="nameList">Kyriakos C. Chatzidimitriou</li>
               <li class="nameList Last">Andreas L. Symeonidis</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The full integration of online repositories in contemporary software development promotes
                     remote work and collaboration. Apart from the apparent benefits, online repositories
                     offer a deluge of data that can be utilized to monitor and improve the software development
                     process. Towards this direction, we have designed and implemented a platform that
                     analyzes data from GitHub in order to compute a series of metrics that quantify the
                     contributions of project collaborators, both from a development as well as an operations
                     (communication) perspective. We analyze contributions throughout the projects' lifecycle
                     and track the number of coding violations, this way aspiring to identify cases of
                     software development that need closer monitoring and (possibly) further actions to
                     be taken. In this context, we have analyzed the 3000 most popular GitHub Java projects
                     and provide the data to the community.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387494">GitterCom: A Dataset of Open Source Developer Communications in Gitter</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Esteban Parra</li>
               <li class="nameList">Ashley Ellis</li>
               <li class="nameList Last">Sonia Haiduc</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Team communication is essential for the development of modern software systems. For
                     distributed software development teams, such as those found in many open source projects,
                     this communication usually takes place using electronic tools. Among these, modern
                     chat platforms such as Gitter are becoming the de facto choice for many software projects
                     due to their advanced features geared towards software development and effective team
                     communication. Gitter channels contain numerous messages exchanged by developers regarding
                     the state of the project, issues and features of the system, team logistics, etc.
                     These messages can contain important information to researchers studying open source
                     software systems, developers new to a particular project and trying to get familiar
                     with the software, etc. Therefore, uncovering what developers are communicating about
                     through Gitter is an essential first step towards successfully understanding and leveraging
                     this information.
                  </p> 
                  <p>We present a new dataset, called GitterCom, which aims to enable research in this
                     direction and represents the largest manually labeled and curated dataset of Gitter
                     developer messages. The dataset is comprised of 10,000 messages collected from 10
                     Gitter communities associated with the development of open source software. Each message
                     was manually annotated and verified by two of the authors, capturing the purpose of
                     the communication expressed by the message. While the dataset has not yet been used
                     in any publication, we discuss how it can enable interesting research opportunities.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387497">Hall-of-Apps: The Top Android Apps Metadata Archive</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Laura Bello-Jiménez</li>
               <li class="nameList">Camilo Escobar-Velásquez</li>
               <li class="nameList">Anamaria Mojica-Hanke</li>
               <li class="nameList">Santiago Cortés-Fernández</li>
               <li class="nameList Last">Mario Linares-Vásquez</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The amount of Android apps available for download is constantly increasing, exerting
                     a continuous pressure on developers to publish outstanding apps. Google Play (GP)
                     is the default distribution channel for Android apps, which provides mobile app users
                     with metrics to identify and report apps quality such as rating, amount of downloads,
                     previous users comments, etc. In addition to those metrics, GP presents a set of top
                     charts that highlight the outstanding apps in different categories. Both metrics and
                     top app charts help developers to identify whether their development decisions are
                     well valued by the community. Therefore, app presence in these top charts is a valuable
                     information when understanding the features of top-apps. In this paper we present
                     Hall-of-Apps, a dataset containing top charts' apps metadata extracted (weekly) from
                     GP, for 4 different countries, during 30 weeks. The data is presented as (i) raw HTML
                     files, (ii) a MongoDB database with all the information contained in app's HTML files
                     (e.g., app description, category, general rating, etc.), and (iii) data visualizations
                     built with the D3.js framework. A first characterization of the data along with the
                     urls to retrieve it can be found in our online appendix: https://bit.ly/2uIkXs8. Dataset:
                     https://bit.ly/2Wu9qsn
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387491">How Often Do Single-Statement Bugs Occur?: The ManySStuBs4J Dataset</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Rafael-Michael Karampatsis</li>
               <li class="nameList Last">Charles Sutton</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Program repair is an important but difficult software engineering problem. One way
                     to achieve acceptable performance is to focus on classes of simple bugs, such as bugs
                     with single statement fixes, or that match a small set of bug templates. However,
                     it is very difficult to estimate the recall of repair techniques for simple bugs,
                     as there are no datasets about how often the associated bugs occur in code. To fill
                     this gap, we provide a dataset of 153,652 single statement bug-fix changes mined from
                     1,000 popular open-source Java projects, annotated by whether they match any of a
                     set of 16 bug templates, inspired by state-of-the-art program repair techniques. In
                     an initial analysis, we find that about 33% of the simple bug fixes match the templates,
                     indicating that a remarkable number of single-statement bugs can be repaired with
                     a relatively small set of templates. Further, we find that template fitting bugs appear
                     with a frequency of about one bug per 1,600-2,500 lines of code (as measured by the
                     size of the project's latest version). We hope that the dataset will prove a resource
                     for both future work in program repair and studies in empirical software engineering.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387484">JTeC: A Large Collection of Java Test Classes for Test Code Analysis and Processing</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Federico Corò</li>
               <li class="nameList">Roberto Verdecchia</li>
               <li class="nameList">Emilio Cruciani</li>
               <li class="nameList">Breno Miranda†</li>
               <li class="nameList Last">Antonia Bertolino</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>The recent push towards test automation and test-driven development continues to scale
                     up the dimensions of test code that needs to be maintained, analysed, and processed
                     side-by-side with production code. As a consequence, on the one side regression testing
                     techniques, e.g., for test suite prioritization or test case selection, capable to
                     handle such large-scale test suites become indispensable; on the other side, as test
                     code exposes own characteristics, specific techniques for its analysis and refactoring
                     are actively sought. We present JTeC, a large-scale dataset of test cases that researchers
                     can use for benchmarking the above techniques or any other type of tool expressly
                     targeting test code. JTeC collects more than 2.5M test classes belonging to 31K+ GitHub
                     projects and summing up to more than 430 Million SLOCs of ready-to-use real-world
                     test code.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387485">LogChunks: A Data Set for Build Log Analysis</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Carolin E. Brandt</li>
               <li class="nameList">Annibale Panichella</li>
               <li class="nameList">Andy Zaidman</li>
               <li class="nameList Last">Moritz Beller</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Build logs are textual by-products that a software build process creates, often as
                     part of its Continuous Integration (CI) pipeline. Build logs are a paramount source
                     of information for developers when debugging into and understanding a build failure.
                     Recently, attempts to partly automate this time-consuming, purely manual activity
                     have come up, such as rule- or information-retrieval-based techniques.
                  </p> 
                  <p>We believe that having a common data set to compare different build log analysis techniques
                     will advance the research area. It will ultimately increase our understanding of CI
                     build failures. In this paper, we present LogChunks, a collection of 797 annotated
                     Travis CI build logs from 80 GitHub repositories in 29 programming languages. For
                     each build log, LogChunks contains a manually labeled log part (chunk) describing
                     why the build failed. We externally validated the data set with the developers who
                     caused the original build failure.
                  </p> 
                  <p>The width and depth of the LogChunks data set are intended to make it the default
                     benchmark for automated build log analysis techniques.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387493">Software-related Slack Chats with Disentangled Conversations</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Preetha Chatterjee</li>
               <li class="nameList">Kostadin Damevski</li>
               <li class="nameList">Nicholas A. Kraft</li>
               <li class="nameList Last">Lori Pollock</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>More than ever, developers are participating in public chat communities to ask and
                     answer software development questions. With over ten million daily active users, Slack
                     is one of the most popular chat platforms, hosting many active channels focused on
                     software development technologies, e.g., python, react. Prior studies have shown that
                     public Slack chat transcripts contain valuable information, which could provide support
                     for improving automatic software maintenance tools or help researchers understand
                     developer struggles or concerns.
                  </p> 
                  <p>In this paper, we present a dataset of software-related Q&amp;A chat conversations, curated
                     for two years from three open Slack communities (python, clojure, elm). Our dataset
                     consists of 38,955 conversations, 437,893 utterances, contributed by 12,171 users.
                     We also share the code for a customized machine-learning based algorithm that automatically
                     extracts (or disentangles) conversations from the downloaded chat transcripts.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387488">TestRoutes: A Manually Curated Method Level Dataset for Test-to-Code Traceability</a></h3>
            <ul class="DLauthors">
               <li class="nameList">András Kicsi</li>
               <li class="nameList">László Vidács</li>
               <li class="nameList Last">Tibor Gyimóthy</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>High test-to-code traceability can be an important aspect of quality assurance and
                     can contribute to bug localization and code maintenance. Several existing techniques
                     and a considerable effort from the scientific community already made significant advances
                     in the field. Despite this, readily accessible data on traceability links is very
                     scarce. To contribute to related research, we present a manually curated test-to-code
                     traceability dataset containing the traceability information on 220 test cases. This
                     method-level data was gathered from 4 open-source software systems written in the
                     Java language, distinguishing not only focal information on test cases but also highlighting
                     the utilized helper methods on both the test and production aspects of code. The data
                     includes more than 2000 of such method classifications.
                  </p>
                  	
               </div>
            </div>
            						
            					
            <h2>SESSION: Registered Reports</h2>
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387507">An Empirical Study on the Impact of Deimplicitization on Comprehension in Programs
                  Using Application Frameworks</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Jürgen Cito</li>
               <li class="nameList">Jiasi Shen</li>
               <li class="nameList Last">Martin Rinard</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Background: Application frameworks, such as Ruby on Rails, introduce abstractions
                     with the goal of simplifying development for particular application domains, such
                     as web development. While experts enjoy increased productivity due to these abstractions,
                     the flow of the programs is often hard to understand for non-experts and newcomers
                     due to implicit flow and concealed lower level action that seems like "magic".
                  </p> 
                  <p>Objective: We conjecture that converting these implicit flows into an explicit and
                     unified form can help non-experts comprehend the programs using these frameworks.
                     We call the process of unifying distributed, implicit flows into a single routine
                     deimplicitization.
                  </p> 
                  <p>Method: We want to conduct an experiment that studies the impact of deimplicitization
                     on program comprehension. Particularly, we want to study how software developers with
                     different expertise (novices/students, framework experts/professional developers)
                     can answer comprehension questions differently with respect to time and correctness,
                     under the treatments of either a deimplicitized version of the program in Python or
                     the original version of the program in Ruby on Rails.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387506">Determining the Intrinsic Structure of Public Software Development History</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Antoine Pietri</li>
               <li class="nameList">Guillaume Rousseau</li>
               <li class="nameList Last">Stefano Zacchiroli</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Background. Collaborative software development has produced a wealth of version control
                     system (VCS) data that can now be analyzed in full. Little is known about the intrinsic
                     structure of the entire corpus of publicly available VCS as an interconnected graph.
                     Understanding its structure is needed to determine the best approach to analyze it
                     in full and to avoid methodological pitfalls when doing so.
                  </p> 
                  <p>Objective. We intend to determine the most salient network topology properties of
                     public software development history as captured by VCS. We will explore: degree distributions,
                     determining whether they are scale-free or not; distribution of connect component
                     sizes; distribution of shortest path lengths.
                  </p> 
                  <p>Method. We will use Software Heritage---which is the largest corpus of public VCS
                     data---compress it using webgraph compression techniques, and analyze it in-memory
                     using classic graph algorithms. Analyses will be performed both on the full graph
                     and on relevant subgraphs.
                  </p> 
                  <p>Limitations. The study is exploratory in nature; as such no hypotheses on the findings
                     is stated at this time. Chosen graph algorithms are expected to scale to the corpus
                     size, but it will need to be confirmed experimentally. External validity will depend
                     on how representative Software Heritage is of the software commons.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387509">Do Explicit Review Strategies Improve Code Review Performance?</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Pavlína Wurzel Gonçalves</li>
               <li class="nameList">Enrico Fregnan</li>
               <li class="nameList">Tobias Baum</li>
               <li class="nameList">Kurt Schneider</li>
               <li class="nameList Last">Alberto Bacchelli</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Context: Code review is a fundamental, yet expensive part of software engineering.
                     Therefore, research on understanding code review and its efficiency and performance
                     is paramount.
                  </p> 
                  <p>Objective: We aim to test the effect of a guidance approach on review effectiveness
                     and efficiency. This effect is expected to work by lowering the cognitive load of
                     the task; thus, we analyze the mediation relationship as well.
                  </p> 
                  <p>Method: To investigate this effect, we employ an experimental design where professional
                     developers have to perform three code reviews. We use three conditions: no guidance,
                     a checklist, and a checklist-based review strategy. Furthermore, we measure the reviewers'
                     cognitive load.
                  </p> 
                  <p>Limitations: The main limitations of this study concern the specific cohort of participants,
                     the mono-operation bias for the guidance conditions, and the generalizability to other
                     changes and defects. Full registered report: https://doi.org/10.17605/OSF.IO/5FPTJ;
                     Materials: https://doi.org/10.6084/m9.figshare.11806656
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387504">Large-Scale Manual Validation of Bugfixing Changes</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Steffen Herbold</li>
               <li class="nameList">Alexander Trautsch</li>
               <li class="nameList Last">Benjamin Ledel</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Context: Accurate data about bug fixes is important for different venues of research,
                     e.g., program repair. While automated procedures are able to identify bug fixing activities,
                     they cannot distinguish between the bug fix and other activities that are happening
                     in parallel, e.g., refactorings or the addition of features. Objective: The creation
                     of a large corpus of manually validated bug fixes and to gain insights into the limitations
                     of manual validation. Method: We use a crowd working approach to manually validate
                     bug fixing commit and analyze the limitations. Limitations: Insights limited to the
                     Java programming language and possibly by the participants in the crowd working.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387508">Multi-language Design Smells: A Backstage Perspective</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Mouna Abidi</li>
               <li class="nameList">Moses Openja</li>
               <li class="nameList Last">Foutse Khomh</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Context: Multi-language systems became prevalent with technological advances. Developers
                     opt for the combination of programming languages to build an application. Problem:
                     Software quality is achieved by following good practices and avoiding bad ones. However,
                     most of the practices in the literature are applied to a single programming language
                     and do not consider the interaction between programming languages. Objective: We previously
                     defined a catalog of bad practices i.e., design smells related to multi-language systems.
                     This paper aims to provide empirical evidence on the relevance of our catalog and
                     its impact on software quality. Method: We analysed 262 snapshots of nine open source
                     projects to detect occurrences of multi-language design smells. We also extracted
                     information about the developers that contributed to those systems. We plan to perform
                     an open and a closed survey targeting developers in general but also developers that
                     contributed to those systems. We will survey developers about the perceived prevalence
                     of those smells, their severity and impact on software quality attributes.
                  </p>
                  	
               </div>
            </div>
            						
            					
            						
            <h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" href="https://dl.acm.org/doi/10.1145/3379597.3387505">The Impact of Dynamics of Collaborative Software Engineering on Introverts: A Study Protocol</a></h3>
            <ul class="DLauthors">
               <li class="nameList">Ingrid Nunes</li>
               <li class="nameList">Christoph Treude</li>
               <li class="nameList Last">Fabio Calefato</li>
            </ul>
            <div class="DLabstract">
               <div style="display:inline">
                  		
                  <p>Background: Collaboration among software engineers through face-to-face discussions
                     in teams has been promoted since the adoption of agile methods. However, these discussions
                     might demote the contribution of software engineers who are introverts, possibly leading
                     to sub-optimal solutions and creating work environments that benefit extroverts. Objective:
                     We aim to evaluate whether providing software engineers with time to work individually
                     and reason about a collective problem is a setting that makes introverts more comfortable
                     to interact and contribute more, ultimately leading to better solutions. Method: We
                     plan to conduct a between-subjects study, with teams in a control group that design
                     a software architecture in a team discussion meeting and teams in a treatment group
                     in which subjects work individually before engaging in a meeting. We will assess and
                     compare the amount of contribution of introverts, their subjective experiences, and
                     the designed solutions. Limitations: As extroverts will be present in both groups,
                     we will not be able to conclude that better solutions are solely due to the increased
                     participation of introverts. The analyses of their subjective experience and amount
                     of contributions might provide evidence to suggest the reasons for observed differences.
                  </p>
                  	
               </div>
            </div>
            						
            					
         </div>
      </div>
   </body>
</html>